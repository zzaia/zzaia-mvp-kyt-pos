{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Know Your Transaction (KYT) - Real-Time Transaction Risk Scoring Engine\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook presents a comprehensive implementation of a Real-Time Transaction Risk Scoring Engine for Anti-Money Laundering (AML) compliance in cryptocurrency transactions. The project addresses the critical need for sub-second risk assessment of Bitcoin transactions by combining traditional AML indicators with blockchain-specific risk factors.\n",
    "\n",
    "### Domain Context: Financial AML for Transactions\n",
    "\n",
    "#### Core Domain Definition\n",
    "Anti-Money Laundering (AML) for transactions encompasses the comprehensive framework of laws, regulations, procedures, and technological solutions designed to prevent criminals from disguising illegally obtained funds as legitimate income through the global financial system. This domain includes detection, prevention, and reporting of money laundering, terrorist financing, tax evasion, market manipulation, and misuse of public funds.\n",
    "\n",
    "### Problem Definition: Real-Time Transaction Risk Classification Engine\n",
    "\n",
    "#### Problem Statement\n",
    "Develop a system that assigns risk classifications to cryptocurrency transactions in real-time, integrating traditional AML indicators with blockchain-specific risk factors including wallet clustering, transaction graph analysis, and counterparty reputation scoring.\n",
    "\n",
    "#### Technical Requirements\n",
    "- **Problem Type**: Classification \n",
    "- **Processing Speed**: Sub-second analysis for high-frequency transactions\n",
    "- **Difficulty Level**: High - requires complex multi-dimensional data processing\n",
    "- **Output Format**: Risk binary classification (illicit/licit)\n",
    "\n",
    "#### Data Landscape\n",
    "The system processes multiple data dimensions:\n",
    "- Transaction metadata (amounts, timestamps, fees)\n",
    "- Wallet addresses and clustering information\n",
    "- Transaction graph relationships and network topology\n",
    "- Counterparty databases and reputation scores\n",
    "- Sanctions lists and regulatory databases\n",
    "- Temporal patterns and behavioral baselines\n",
    "\n",
    "### References\n",
    "\n",
    "This notebook implementation is based on the comprehensive research and analysis conducted during the project development phase. The following reference documents were used in the composition of this initial description:\n",
    "\n",
    "- **Domain Research**: [current-domain.md](domains/current-domain.md) - Contains detailed market analysis, regulatory framework research, and commercial viability assessment for the Financial AML domain\n",
    "- **Problem Analysis**: [current-problem.md](problems/current-problem.md) - Provides comprehensive problem refinement, technical requirements analysis, and solution approach evaluation\n",
    "- **Dataset Evaluation**: [current-dataset.md](datasets/current-dataset.md) - Documents dataset selection criteria, suitability scoring, and detailed feature analysis for the Elliptic dataset\n",
    "- **Dataset Analysis & Preprocessing**: [dataset-analysis-and-preprocessing.ipynb](datasets/scripts/dataset-analysis-and-preprocessing.ipynb) - Comprehensive Jupyter notebook containing Elliptic dataset download, exploratory data analysis, feature engineering, preprocessing pipeline, and ML preparation steps\n",
    "\n",
    "These reference documents contain the foundational research that informed the technical approach, feature engineering strategy, and implementation decisions reflected in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the primary entry point for the MVP KYT implementation, providing both technical implementation and business context for real-time cryptocurrency transaction risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Comprehensive import of all required libraries for machine learning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-processed datasets\n",
    "\n",
    "The pre-processing step reduced the dimensionality from 166 features to only 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from HDF5: (203769, 48) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (46564, 48) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (157205, 48) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (234355, 2) - All subsequent operations will use compressed data\n",
      "\n",
      "📊 Dataset Summary:\n",
      "  - Features: 203,769 transactions × 46 features\n",
      "  - Labeled: 46,564 transactions\n",
      "  - Unlabeled: 157,205 transactions\n",
      "  - Edges: 234,355 transaction relationships\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The complete dataset already pre-processed \n",
    "df_complete = pd.read_hdf(\"./datasets/processed/df_complete.h5\", key=\"df_complete\")\n",
    "print(f\"Loaded from HDF5: {df_complete.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered labeled dataset already pre-processed\n",
    "df_labeled = pd.read_hdf(\"./datasets/processed/df_labeled.h5\", key=\"df_labeled\")\n",
    "print(f\"Loaded from HDF5: {df_labeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered unlabeled dataset already pre-processed\n",
    "df_unlabeled = pd.read_hdf(\"./datasets/processed/df_unlabeled.h5\", key=\"df_unlabeled\")\n",
    "print(f\"Loaded from HDF5: {df_unlabeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The edges dataset that maps relationships between transaction nodes\n",
    "df_edges = pd.read_hdf(\"./datasets/processed/df_edges.h5\", key=\"df_edges\")\n",
    "print(f\"Loaded from HDF5: {df_edges.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "\n",
    "# Summary of all datasets\n",
    "print(f\"\\n📊 Dataset Summary:\")\n",
    "print(f\"  - Features: {df_complete.shape[0]:,} transactions × {df_complete.shape[1] -2} features\")\n",
    "print(f\"  - Labeled: {df_labeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Unlabeled: {df_unlabeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Edges: {df_edges.shape[0]:,} transaction relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Strategy\n",
    "\n",
    "Let's try three machine learning approaches with the data and compare than, each methodology will have it's winning model. \n",
    "\n",
    "1. Train models using the supervised methodology using only the labeled sub-dataset;\n",
    "2. Train models using the non-supervised approach using the complete dataset ignoring the edge dataset; **[FUTURE DEVELOPMENT]**\n",
    "3. Train models using the non-supervised approach using the complete dataset taking to account the relationship between those transactions, by using the edge dataset. **[FUTURE DEVELOPMENT]**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train models using the supervised methodology\n",
    "\n",
    "Let`s prepare the dataset for training and validation\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some parameters\n",
    "np.random.seed(7)\n",
    "\n",
    "# Prepare data (df_labeled already loaded: 46,564 × 48)\n",
    "seed = 7 # random seed\n",
    "test_size = 0.20 # test set size\n",
    "X = df_labeled.drop(['class', 'txId'], axis=1)  # 46 features\n",
    "y = df_labeled['class']  # Binary target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # stratified holdout\n",
    "\n",
    "# Cross-validation setup\n",
    "n_splits = 10 # PARAMETER: number of folds\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define which models to use\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the individual models\n",
    "reg = ('LR', LogisticRegression(max_iter=200))\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "cart = ('CART', DecisionTreeClassifier())\n",
    "naive = ('NB', GaussianNB())\n",
    "svm = ('SVM', SVC())\n",
    "\n",
    "models = []\n",
    "models.append(reg)\n",
    "models.append(knn)\n",
    "models.append(cart)\n",
    "models.append(naive)\n",
    "models.append(svm)\n",
    "\n",
    "# Defining ensemble models\n",
    "bagging = ('Bag', BaggingClassifier())\n",
    "forest = ('RF', RandomForestClassifier())\n",
    "extra = ('ET', ExtraTreesClassifier())\n",
    "ada = ('Ada', AdaBoostClassifier())\n",
    "gradient = ('GB', GradientBoostingClassifier())\n",
    "voting = ('Voting', VotingClassifier(models))\n",
    "\n",
    "# Creating the pipelines\n",
    "pipelines = []\n",
    "\n",
    "# Defining the pipelines, for future experimentation \n",
    "pipelines.append(('LR', Pipeline([reg]))) \n",
    "#pipelines.append(('KNN', Pipeline([knn])))\n",
    "#pipelines.append(('CART', Pipeline([cart])))\n",
    "#pipelines.append(('NB', Pipeline([naive])))\n",
    "#pipelines.append(('SVM', Pipeline([svm])))\n",
    "#pipelines.append(('Bag', Pipeline([bagging])))\n",
    "#pipelines.append(('RF', Pipeline([forest])))\n",
    "#pipelines.append(('ET', Pipeline([extra])))\n",
    "#pipelines.append(('Ada', Pipeline([ada])))\n",
    "#pipelines.append(('GB', Pipeline([gradient])))\n",
    "#pipelines.append(('Vot', Pipeline([voting])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the parameter distributions for a random grid search\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'LR': {\n",
    "        'LR__C': uniform(0.01, 100),                    # Regularization strength (inverse)\n",
    "        'LR__solver': ['liblinear', 'saga'],            # Optimization algorithm (compatible with l1/l2)\n",
    "        'LR__penalty': ['l1', 'l2', 'none'],            # Regularization type (removed elasticnet)\n",
    "        'LR__max_iter': [200, 500, 1000]               # Maximum iterations for convergence\n",
    "    },\n",
    "    'KNN': {\n",
    "        'KNN__n_neighbors': randint(3, 21),             # Number of nearest neighbors to consider\n",
    "        'KNN__weights': ['uniform', 'distance'],        # Weight function for neighbors\n",
    "        'KNN__metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metric\n",
    "        'KNN__p': randint(1, 3)                         # Power parameter for minkowski metric\n",
    "    },\n",
    "    'CART': {\n",
    "        'CART__max_depth': randint(3, 20),               # Maximum tree depth (prevent overfitting)\n",
    "        'CART__min_samples_split': randint(2, 20),       # Min samples required to split node\n",
    "        'CART__min_samples_leaf': randint(1, 10),        # Min samples required at leaf node\n",
    "        'CART__criterion': ['gini', 'entropy']           # Split quality measure\n",
    "    },\n",
    "    'NB': {\n",
    "        'NB__var_smoothing': uniform(1e-12, 1e-6),      # Laplace smoothing for numerical stability\n",
    "        'NB__priors': [None]                             # Class probabilities (let model learn from data)\n",
    "    },\n",
    "    'SVM': {\n",
    "        'SVM__C': uniform(0.1, 100),                    # Regularization parameter (margin vs errors)\n",
    "        'SVM__kernel': ['rbf', 'poly', 'sigmoid'],      # Kernel function type\n",
    "        'SVM__gamma': ['scale', 'auto']                 # Kernel coefficient influence\n",
    "    },\n",
    "    'RF': {\n",
    "        'RF__n_estimators': randint(50, 300),           # Number of trees in forest\n",
    "        'RF__max_depth': randint(3, 20),                # Maximum depth of each tree\n",
    "        'RF__min_samples_split': randint(2, 20),        # Min samples to split internal node\n",
    "        'RF__min_samples_leaf': randint(1, 10),         # Min samples at leaf node\n",
    "        'RF__max_features': ['sqrt', 'log2', None],     # Features to consider for best split\n",
    "        'RF__bootstrap': [True, False]                  # Whether to use bootstrap samples\n",
    "    },\n",
    "    'ET': {\n",
    "        'ET__n_estimators': randint(50, 300),           # Number of trees in ensemble\n",
    "        'ET__max_depth': randint(3, 20),                # Maximum depth of trees\n",
    "        'ET__min_samples_split': randint(2, 20),        # Min samples to split node\n",
    "        'ET__min_samples_leaf': randint(1, 10),         # Min samples at leaf\n",
    "        'ET__max_features': ['sqrt', 'log2', None],     # Random feature subset size\n",
    "        'ET__bootstrap': [True, False]                  # Bootstrap sampling toggle\n",
    "    },\n",
    "    'GB': {\n",
    "        'GB__n_estimators': randint(50, 200),           # Number of boosting stages\n",
    "        'GB__learning_rate': uniform(0.01, 0.3),        # Step size shrinkage (prevent overfitting)\n",
    "        'GB__max_depth': randint(3, 10),                # Maximum depth of individual estimators\n",
    "        'GB__subsample': uniform(0.5, 0.5)              # Fraction of samples for fitting trees (0.5-1.0)\n",
    "    },\n",
    "    'Ada': {\n",
    "        'Ada__n_estimators': randint(50, 200),          # Maximum number of weak learners\n",
    "        'Ada__learning_rate': uniform(0.1, 0.9),        # Weight applied to each classifier (0.1-1.0)\n",
    "        'Ada__algorithm': ['SAMME', 'SAMME.R']          # Boosting algorithm variant\n",
    "    },\n",
    "    'Bag': {\n",
    "        'Bag__n_estimators': randint(50, 200),          # Number of base estimators\n",
    "        'Bag__max_samples': uniform(0.5, 0.5),          # Fraction of samples for each estimator\n",
    "        'Bag__max_features': uniform(0.5, 0.5)          # Fraction of features for each estimator\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the training phase using the random grid search with cross validation and rank the best models by accuracy.\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_iter = 50  # PARAMETER: Number of parameter combinations to try\n",
    "scoring = 'accuracy' # PARAMETER: Scoring metric\n",
    "n_jobs = -1  # PARAMETER: Use all available cores\n",
    "verbosity = 0  # PARAMETER: Verbosity level\n",
    "\n",
    "optimized_models = []\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "print(\"🔍 Training Models with RandomizedSearchCV Optimization...\")\n",
    "print(f\"Training {len(pipelines)} models: Basic + Ensemble methods\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in pipelines:\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    # Use RandomizedSearchCV for models with parameters definitions\n",
    "    if name in param_distributions and param_distributions[name]:\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_distributions[name],\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            verbose=verbosity,\n",
    "            scoring=scoring,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        best_score = random_search.best_score_\n",
    "        std_score = random_search.cv_results_['std_test_score'][random_search.best_index_]\n",
    "        \n",
    "        print(f\"✅ {best_score:.4f} (±{std_score:.4f})\")\n",
    "    else:\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "        best_model = model\n",
    "        best_score = cv_results.mean()\n",
    "        std_score = cv_results.std()\n",
    "        \n",
    "        print(f\"✅ {best_score:.4f} (±{std_score:.4f})\")\n",
    "    \n",
    "    optimized_models.append((name, best_model))\n",
    "    results.append(best_score)\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save all models\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to save models from training\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    for name, model in optimized_models:\n",
    "        joblib.dump(model, f\"./models/{name}.pkl\", compress=True)\n",
    "    print(f\"💾 Saved {len(optimized_models)} models: {[name for name, _ in optimized_models]}\")\n",
    "except NameError:\n",
    "    # Load models if optimized_models doesn't exist\n",
    "    optimized_models = []\n",
    "    if os.path.exists(\"./models\"):\n",
    "        for file in os.listdir(\"./models\"):\n",
    "            if file.endswith('.pkl'):\n",
    "                name = file.replace('.pkl', '')\n",
    "                model = joblib.load(f\"./models/{file}\")\n",
    "                optimized_models.append((name, model))\n",
    "        print(f\"📁 Loaded {len(optimized_models)} models: {[name for name, _ in optimized_models]}\")\n",
    "    else:\n",
    "        print(\"❌ No models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s select the best models\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏆 FINAL MODEL RANKINGS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort and display results\n",
    "sorted_results = sorted(zip(names, results), key=lambda x: x[1], reverse=True)\n",
    "for i, (name, score) in enumerate(sorted_results, 1):\n",
    "    emoji = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else \"  \"\n",
    "    print(f\"{emoji} {i:2d}. {name:<4}: {score:.4f}\")\n",
    "\n",
    "best_model_name = sorted_results[0][0]\n",
    "best_model_score = sorted_results[0][1]\n",
    "print(f\"\\n🏆 Champion Model: {best_model_name} ({best_model_score:.4f})\")\n",
    "\n",
    "# Create best_optimized_models with top 3 models\n",
    "best_optimized_models = []\n",
    "for name, score in sorted_results[:3]:\n",
    "    # Find the corresponding model from optimized_models\n",
    "    for opt_name, opt_model in optimized_models:\n",
    "        if opt_name == name:\n",
    "            best_optimized_models.append((opt_name, opt_model))\n",
    "            break\n",
    "\n",
    "print(f\"\\n🎯 Top 3 models selected: {[name for name, _ in best_optimized_models]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the best models\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s apply the model into unknown data\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train models using the non-supervised approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train models using the non-supervised approach using the edge dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-analytics-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
