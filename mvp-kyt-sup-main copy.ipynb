{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Know Your Transaction (KYT) - Real-Time Transaction Risk Scoring Engine\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook presents a comprehensive implementation of a Real-Time Transaction Risk Scoring Engine for Anti-Money Laundering (AML) compliance in cryptocurrency transactions. The project addresses the critical need for sub-second risk assessment of Bitcoin transactions by combining traditional AML indicators with blockchain-specific risk factors.\n",
    "\n",
    "### Domain Context: Financial AML for Transactions\n",
    "\n",
    "#### Core Domain Definition\n",
    "Anti-Money Laundering (AML) for transactions encompasses the comprehensive framework of laws, regulations, procedures, and technological solutions designed to prevent criminals from disguising illegally obtained funds as legitimate income through the global financial system. This domain includes detection, prevention, and reporting of money laundering, terrorist financing, tax evasion, market manipulation, and misuse of public funds.\n",
    "\n",
    "### Problem Definition: Real-Time Transaction Risk Classification Engine\n",
    "\n",
    "#### Problem Statement\n",
    "Develop a system that assigns risk classifications to cryptocurrency transactions in real-time, integrating traditional AML indicators with blockchain-specific risk factors including wallet clustering, transaction graph analysis, and counterparty reputation scoring.\n",
    "\n",
    "#### Technical Requirements\n",
    "- **Problem Type**: Classification \n",
    "- **Processing Speed**: Sub-second analysis for high-frequency transactions\n",
    "- **Difficulty Level**: High - requires complex multi-dimensional data processing\n",
    "- **Output Format**: Risk binary classification (illicit/licit)\n",
    "\n",
    "#### Data Landscape\n",
    "The system processes multiple data dimensions:\n",
    "- Transaction metadata (amounts, timestamps, fees)\n",
    "- Wallet addresses and clustering information\n",
    "- Transaction graph relationships and network topology\n",
    "- Counterparty databases and reputation scores\n",
    "- Sanctions lists and regulatory databases\n",
    "- Temporal patterns and behavioral baselines\n",
    "\n",
    "### References\n",
    "\n",
    "This notebook implementation is based on the comprehensive research and analysis conducted during the project development phase. The following reference documents were used in the composition of this initial description:\n",
    "\n",
    "- **Domain Research**: [current-domain.md](domains/current-domain.md) - Contains detailed market analysis, regulatory framework research, and commercial viability assessment for the Financial AML domain\n",
    "- **Problem Analysis**: [current-problem.md](problems/current-problem.md) - Provides comprehensive problem refinement, technical requirements analysis, and solution approach evaluation\n",
    "- **Dataset Evaluation**: [current-dataset.md](datasets/current-dataset.md) - Documents dataset selection criteria, suitability scoring, and detailed feature analysis for the Elliptic dataset\n",
    "- **Dataset Analysis & Preprocessing**: [dataset-analysis-and-preprocessing.ipynb](datasets/scripts/dataset-analysis-and-preprocessing.ipynb) - Comprehensive Jupyter notebook containing Elliptic dataset download, exploratory data analysis, feature engineering, preprocessing pipeline, and ML preparation steps\n",
    "\n",
    "These reference documents contain the foundational research that informed the technical approach, feature engineering strategy, and implementation decisions reflected in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the primary entry point for the MVP KYT implementation, providing both technical implementation and business context for real-time cryptocurrency transaction risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Comprehensive import of all required libraries for machine learning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform, loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-processed datasets\n",
    "\n",
    "The pre-processing step reduced the dimensionality from 166 features to only 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from HDF5: (203769, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (46564, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (157205, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (234355, 2) - All subsequent operations will use compressed data\n",
      "\n",
      "ðŸ“Š Dataset Summary:\n",
      "  - Features: 203,769 transactions Ã— 166 features\n",
      "  - Labeled: 46,564 transactions\n",
      "  - Unlabeled: 157,205 transactions\n",
      "  - Edges: 234,355 transaction relationships\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The complete dataset already pre-processed \n",
    "df_complete = pd.read_hdf(\"./datasets/processed/df_complete.h5\", key=\"df_complete\")\n",
    "print(f\"Loaded from HDF5: {df_complete.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered labeled dataset already pre-processed\n",
    "df_labeled = pd.read_hdf(\"./datasets/processed/df_labeled.h5\", key=\"df_labeled\")\n",
    "print(f\"Loaded from HDF5: {df_labeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered unlabeled dataset already pre-processed\n",
    "df_unlabeled = pd.read_hdf(\"./datasets/processed/df_unlabeled.h5\", key=\"df_unlabeled\")\n",
    "print(f\"Loaded from HDF5: {df_unlabeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The edges dataset that maps relationships between transaction nodes\n",
    "df_edges = pd.read_hdf(\"./datasets/processed/df_edges.h5\", key=\"df_edges\")\n",
    "print(f\"Loaded from HDF5: {df_edges.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "\n",
    "# Summary of all datasets\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"  - Features: {df_complete.shape[0]:,} transactions Ã— {df_complete.shape[1] -2} features\")\n",
    "print(f\"  - Labeled: {df_labeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Unlabeled: {df_unlabeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Edges: {df_edges.shape[0]:,} transaction relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Strategy\n",
    "\n",
    "Let's apply the machine learning technics:\n",
    "\n",
    "1. Define overall parameters and make data splits\n",
    "2. Defining all training models to be used\n",
    "3. Defining all pipelines to be used during training\n",
    "4. Defining model parameters distribution for a optimization search \n",
    "5. Defining the object function and metrics to optimize\n",
    "6. Execute the optimized training\n",
    "7. Save all resulting models\n",
    "8. Validate all models and select the best models\n",
    "9. Use best models to predict unknown data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Define overall parameters and make data splits**\n",
    "\n",
    "Let`s prepare the dataset for training and validation\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining overall parameters\n",
    "random_seed = 4354 # PARAMETER: random seed\n",
    "test_size_split = 0.20 # PARAMETER: test set size\n",
    "n_stratified_splits = 2 # PARAMETER: number of folds\n",
    "n_pca_components = 0.95 # PARAMETER: PCA components to keep\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Prepare data (df_labeled already loaded: 46,564 Ã— 48)\n",
    "x_labeled = df_labeled.drop(['class', 'txId'], axis=1)  # 46 features\n",
    "y_labeled = df_labeled['class']  # Binary target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_labeled, y_labeled,\n",
    "    test_size=test_size_split, \n",
    "    shuffle=True, \n",
    "    random_state=random_seed, \n",
    "    stratify=y_labeled) # stratified holdout\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=n_stratified_splits, \n",
    "                     shuffle=True, \n",
    "                     random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Defining all training models to be used**\n",
    "\n",
    "Let's define which models to use\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the individual models\n",
    "reg = ('LR', LogisticRegression())\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "cart = ('CART', DecisionTreeClassifier())\n",
    "naive = ('NB', GaussianNB())\n",
    "svm = ('SVM', SVC())\n",
    "\n",
    "models = []\n",
    "models.append(reg)\n",
    "models.append(knn)\n",
    "models.append(cart)\n",
    "models.append(naive)\n",
    "models.append(svm)\n",
    "\n",
    "# Defining ensemble models\n",
    "bagging = ('Bag', BaggingClassifier())\n",
    "forest = ('RF', RandomForestClassifier())\n",
    "extra = ('ET', ExtraTreesClassifier())\n",
    "ada = ('Ada', AdaBoostClassifier())\n",
    "gradient = ('GB', GradientBoostingClassifier())\n",
    "voting = ('Voting', VotingClassifier(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Defining all pipelines to be used during training**\n",
    "\n",
    "Let's define which ML pipelines to use during training.\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]\n",
    "\n",
    "This section focuses on selecting the most relevant features for distinguishing between licit (class 1) and illicit (class 2) transactions. We are aiming to reduce the feature dimensionality at the same time as maximizing the dissimilarity of the original dataset, thus extracting the most discriminative features and improving the model training performance. Let`s apply the following technics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipelines\n",
    "pipelines = []\n",
    "std  = ('std', StandardScaler())  # Standardization\n",
    "pca = ('pca', PCA(n_components=n_pca_components))  # Feature reduction\n",
    "\n",
    "# Defining the pipelines, for future experimentation \n",
    "pipelines.append(('LR', Pipeline([std, pca, reg]))) \n",
    "pipelines.append(('KNN', Pipeline([std, pca, knn])))\n",
    "pipelines.append(('CART', Pipeline([std, pca, cart])))\n",
    "pipelines.append(('NB', Pipeline([std, pca, naive])))\n",
    "pipelines.append(('SVM', Pipeline([std, pca, svm])))\n",
    "pipelines.append(('Bag', Pipeline([std, pca, bagging])))\n",
    "pipelines.append(('RF', Pipeline([std, pca, forest])))\n",
    "pipelines.append(('ET', Pipeline([std, pca, extra])))\n",
    "pipelines.append(('Ada', Pipeline([std, pca, ada])))\n",
    "pipelines.append(('GB', Pipeline([std, pca, gradient])))\n",
    "#pipelines.append(('Vot', Pipeline([std, pca, voting])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Defining model parameters distribution for a grid search approach**\n",
    "\n",
    "Let's prepare the parameter distributions for a random grid search\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Parameter Suggestions - All original parameters preserved with comments\n",
    "def suggest_params(trial, name):\n",
    "    params = {\n",
    "        'LR': {\n",
    "            'LR__C': trial.suggest_float('LR__C', 1e-4, 1e2, log=True),  # Regularization strength\n",
    "            'LR__solver': trial.suggest_categorical('LR__solver', ['lbfgs', 'newton-cg', 'sag', 'saga']),  # Optimization algorithm\n",
    "            'LR__penalty': trial.suggest_categorical('LR__penalty', ['l2', 'none']),  # Regularization type\n",
    "            'LR__max_iter': trial.suggest_categorical('LR__max_iter', [1000, 2000, 5000]),  # Convergence limit\n",
    "            'LR__tol': trial.suggest_float('LR__tol', 1e-6, 1e-3, log=True)  # Convergence tolerance\n",
    "        },\n",
    "        'KNN': {\n",
    "            'KNN__n_neighbors': trial.suggest_int('KNN__n_neighbors', 3, 21, step=2),  # Number of neighbors\n",
    "            'KNN__weights': trial.suggest_categorical('KNN__weights', ['uniform', 'distance']),  # Weight function\n",
    "            'KNN__metric': trial.suggest_categorical('KNN__metric', ['euclidean', 'manhattan', 'minkowski']),  # Distance metric\n",
    "            'KNN__p': trial.suggest_int('KNN__p', 1, 3)  # Minkowski power parameter\n",
    "        },\n",
    "        'CART': {\n",
    "            'CART__max_depth': trial.suggest_int('CART__max_depth', 3, 20),  # Maximum tree depth\n",
    "            'CART__min_samples_split': trial.suggest_int('CART__min_samples_split', 10, 50),  # Min samples to split\n",
    "            'CART__min_samples_leaf': trial.suggest_int('CART__min_samples_leaf', 5, 20),  # Min samples per leaf\n",
    "            'CART__criterion': trial.suggest_categorical('CART__criterion', ['gini', 'entropy'])  # Split quality measure\n",
    "        },\n",
    "        'NB': {\n",
    "            'NB__var_smoothing': trial.suggest_float('NB__var_smoothing', 1e-12, 1e-6, log=True)  # Variance smoothing\n",
    "        },\n",
    "        'SVM': {\n",
    "            'SVM__C': trial.suggest_float('SVM__C', 1e-2, 1e3, log=True),  # Regularization parameter\n",
    "            'SVM__kernel': trial.suggest_categorical('SVM__kernel', ['rbf', 'poly', 'sigmoid']),  # Kernel function\n",
    "            'SVM__gamma': trial.suggest_categorical('SVM__gamma', ['scale', 'auto']),  # Kernel coefficient\n",
    "            'SVM__probability': True  # Enable probability estimates for AML risk scoring\n",
    "        },\n",
    "        'RF': {\n",
    "            'RF__n_estimators': trial.suggest_int('RF__n_estimators', 100, 500, step=50),  # Number of trees\n",
    "            'RF__max_depth': trial.suggest_int('RF__max_depth', 10, 25),  # Individual tree depth\n",
    "            'RF__min_samples_split': trial.suggest_int('RF__min_samples_split', 5, 20),  # Conservative splitting\n",
    "            'RF__min_samples_leaf': trial.suggest_int('RF__min_samples_leaf', 2, 10),  # Leaf size constraint\n",
    "            'RF__max_features': trial.suggest_categorical('RF__max_features', ['sqrt', 'log2', None]),  # Feature subset\n",
    "            'RF__bootstrap': trial.suggest_categorical('RF__bootstrap', [True, False])  # Bootstrap sampling\n",
    "        },\n",
    "        'ET': {\n",
    "            'ET__n_estimators': trial.suggest_int('ET__n_estimators', 100, 500, step=50),  # Number of trees\n",
    "            'ET__max_depth': trial.suggest_int('ET__max_depth', 10, 25),  # Maximum depth\n",
    "            'ET__min_samples_split': trial.suggest_int('ET__min_samples_split', 5, 20),  # Min samples to split\n",
    "            'ET__min_samples_leaf': trial.suggest_int('ET__min_samples_leaf', 2, 10),  # Min samples at leaf\n",
    "            'ET__max_features': trial.suggest_categorical('ET__max_features', ['sqrt', 'log2', None]),  # Feature subset\n",
    "            'ET__bootstrap': trial.suggest_categorical('ET__bootstrap', [True, False])  # Bootstrap sampling\n",
    "        },\n",
    "        'GB': {\n",
    "            'GB__n_estimators': trial.suggest_int('GB__n_estimators', 100, 300, step=25),  # Boosting stages\n",
    "            'GB__learning_rate': trial.suggest_float('GB__learning_rate', 1e-2, 3e-1, log=True),  # Learning rate\n",
    "            'GB__max_depth': trial.suggest_int('GB__max_depth', 3, 8),  # Individual tree depth\n",
    "            'GB__subsample': trial.suggest_float('GB__subsample', 0.7, 0.9)  # Subsample fraction\n",
    "        },\n",
    "        'Ada': {\n",
    "            'Ada__n_estimators': trial.suggest_int('Ada__n_estimators', 50, 200, step=25),  # Number of weak learners\n",
    "            'Ada__learning_rate': trial.suggest_float('Ada__learning_rate', 0.5, 1.5),  # Learning rate\n",
    "            'Ada__algorithm': trial.suggest_categorical('Ada__algorithm', ['SAMME', 'SAMME.R'])  # AdaBoost algorithm\n",
    "        },\n",
    "        'Bag': {\n",
    "            'Bag__n_estimators': trial.suggest_int('Bag__n_estimators', 50, 200, step=25),  # Number of estimators\n",
    "            'Bag__max_samples': trial.suggest_float('Bag__max_samples', 0.6, 0.9),  # Sample fraction\n",
    "            'Bag__max_features': trial.suggest_float('Bag__max_features', 0.7, 1.0)  # Feature fraction\n",
    "        }\n",
    "    }\n",
    "    return params.get(name, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Defining the object function and metrics to optimize**\n",
    "\n",
    "Let`s define which metrics to optimize in the training search\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric objective function for AML/KYT systems\n",
    "def aml_objective(y_true, y_pred, y_proba=None):\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba) if y_proba is not None and len(np.unique(y_true)) > 1 else 0.5\n",
    "    \n",
    "    # Weighted combination: Recall(35%) + Precision(25%) + F1(25%) + AUC(15%)\n",
    "    return 0.35*recall + 0.25*precision + 0.25*f1 + 0.15*auc\n",
    "\n",
    "# Create sklearn-compatible scorer for cross_val_score\n",
    "def aml_scorer_func(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    try:\n",
    "        y_proba = estimator.predict_proba(X)[:, 1]\n",
    "    except:\n",
    "        y_proba = None\n",
    "    return aml_objective(y, y_pred, y_proba)\n",
    "\n",
    "aml_scorer = make_scorer(aml_scorer_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Execute the optimized training**\n",
    "\n",
    "Let's execute the training phase using the random grid search with cross validation and rank the best models by accuracy.\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:42:35,292] A new study created in memory with name: no-name-70f26644-78d7-4b8f-8871-100cec9063dd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Optuna + cross_val_score Model Optimization\n",
      "----------------------------------------------------------------------\n",
      "Training LR... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869f18dce23d4bb9af859c65662f9ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-21 17:42:35,546] Trial 0 failed with parameters: {'LR__C': 0.0015309100761935702, 'LR__solver': 'lbfgs', 'LR__penalty': 'none', 'LR__max_iter': 1000, 'LR__tol': 0.00010994048924701591, 'KNN__n_neighbors': 15, 'KNN__weights': 'distance', 'KNN__metric': 'manhattan', 'KNN__p': 2, 'CART__max_depth': 16, 'CART__min_samples_split': 30, 'CART__min_samples_leaf': 20, 'CART__criterion': 'entropy', 'NB__var_smoothing': 4.065685518531922e-09, 'SVM__C': 3.234473744318124, 'SVM__kernel': 'sigmoid', 'SVM__gamma': 'scale', 'RF__n_estimators': 350, 'RF__max_depth': 24, 'RF__min_samples_split': 13, 'RF__min_samples_leaf': 7, 'RF__max_features': 'sqrt', 'RF__bootstrap': False, 'ET__n_estimators': 200, 'ET__max_depth': 12, 'ET__min_samples_split': 15, 'ET__min_samples_leaf': 4, 'ET__max_features': 'sqrt', 'ET__bootstrap': True, 'GB__n_estimators': 275, 'GB__learning_rate': 0.03905637462116368, 'GB__max_depth': 7, 'GB__subsample': 0.8102200417708098, 'Ada__n_estimators': 100, 'Ada__learning_rate': 0.738534779383393, 'Ada__algorithm': 'SAMME.R', 'Bag__n_estimators': 75, 'Bag__max_samples': 0.7363994652512813, 'Bag__max_features': 0.9145926769185315} because of the following error: ValueError('\\nAll the 2 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n1 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\\n    estimator._validate_params()\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\\n    validate_parameter_constraints(\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        self._parameter_constraints,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        self.get_params(deep=False),\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        caller_name=self.__class__.__name__,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\n    ...<2 lines>...\\n    )\\nsklearn.utils._param_validation.InvalidParameterError: The \\'penalty\\' parameter of LogisticRegression must be a str among {\\'elasticnet\\', \\'l2\\', \\'l1\\'} or None. Got \\'none\\' instead.\\n\\n--------------------------------------------------------------------------------\\n1 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\\n    estimator._validate_params()\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\\n    validate_parameter_constraints(\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        self._parameter_constraints,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        self.get_params(deep=False),\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        caller_name=self.__class__.__name__,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\n    ...<2 lines>...\\n    )\\nsklearn.utils._param_validation.InvalidParameterError: The \\'penalty\\' parameter of LogisticRegression must be a str among {\\'l2\\', \\'l1\\', \\'elasticnet\\'} or None. Got \\'none\\' instead.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_128973/674451905.py\", line 15, in objective\n",
      "    scores = cross_val_score(pipe, X_train, y_train,\n",
      "                             cv=cv,\n",
      "                             scoring=aml_scorer,\n",
      "                             n_jobs=op_n_jobs)\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 677, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "        estimator=estimator,\n",
      "    ...<9 lines>...\n",
      "        error_score=error_score,\n",
      "    )\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 419, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 505, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 2 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "[W 2025-09-21 17:42:35,548] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scores.mean()\n\u001b[32m     20\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     21\u001b[39m                             sampler=TPESampler(seed=random_seed),\n\u001b[32m     22\u001b[39m                             pruner=MedianPruner())\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m study.optimize(objective, n_trials=op_n_trials, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Apply best parameters\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m study.best_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     _optimize(\n\u001b[32m    491\u001b[39m         study=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    492\u001b[39m         func=func,\n\u001b[32m    493\u001b[39m         n_trials=n_trials,\n\u001b[32m    494\u001b[39m         timeout=timeout,\n\u001b[32m    495\u001b[39m         n_jobs=n_jobs,\n\u001b[32m    496\u001b[39m         catch=\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[32m    497\u001b[39m         callbacks=callbacks,\n\u001b[32m    498\u001b[39m         gc_after_trial=gc_after_trial,\n\u001b[32m    499\u001b[39m         show_progress_bar=show_progress_bar,\n\u001b[32m    500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         _optimize_sequential(\n\u001b[32m     64\u001b[39m             study,\n\u001b[32m     65\u001b[39m             func,\n\u001b[32m     66\u001b[39m             n_trials,\n\u001b[32m     67\u001b[39m             timeout,\n\u001b[32m     68\u001b[39m             catch,\n\u001b[32m     69\u001b[39m             callbacks,\n\u001b[32m     70\u001b[39m             gc_after_trial,\n\u001b[32m     71\u001b[39m             reseed_sampler_rng=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m             time_start=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m             progress_bar=progress_bar,\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = _run_trial(study, func, catch)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = func(trial)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     13\u001b[39m params = suggest_params(trial, name)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params: pipe.set_params(**params)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m scores = cross_val_score(pipe, X_train, y_train, \n\u001b[32m     16\u001b[39m                          cv=cv, \n\u001b[32m     17\u001b[39m                          scoring=aml_scorer, \n\u001b[32m     18\u001b[39m                          n_jobs=op_n_jobs)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = cross_validate(\n\u001b[32m    678\u001b[39m     estimator=estimator,\n\u001b[32m    679\u001b[39m     X=X,\n\u001b[32m    680\u001b[39m     y=y,\n\u001b[32m    681\u001b[39m     groups=groups,\n\u001b[32m    682\u001b[39m     scoring={\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: scorer},\n\u001b[32m    683\u001b[39m     cv=cv,\n\u001b[32m    684\u001b[39m     n_jobs=n_jobs,\n\u001b[32m    685\u001b[39m     verbose=verbose,\n\u001b[32m    686\u001b[39m     params=params,\n\u001b[32m    687\u001b[39m     pre_dispatch=pre_dispatch,\n\u001b[32m    688\u001b[39m     error_score=error_score,\n\u001b[32m    689\u001b[39m )\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n"
     ]
    }
   ],
   "source": [
    "# Optuna Bayesian Optimization with cross_val_score\n",
    "op_n_trials = 20  # PARAMETER: Trials per model\n",
    "op_n_jobs = 2  # PARAMETER: Parallel jobs for cross_val_score\n",
    "optimized_models = []\n",
    "results = []\n",
    "\n",
    "print(\"ðŸš€ Optuna + cross_val_score Model Optimization\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, pipe in pipelines:\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    def objective(trial):\n",
    "        params = suggest_params(trial, name)\n",
    "        if params: pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, \n",
    "                                 cv=cv, \n",
    "                                 scoring=aml_scorer, \n",
    "                                 n_jobs=op_n_jobs)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction='maximize', \n",
    "                                sampler=TPESampler(seed=random_seed),\n",
    "                                pruner=MedianPruner())\n",
    "    study.optimize(objective, n_trials=op_n_trials, show_progress_bar=True)\n",
    "    \n",
    "    # Apply best parameters\n",
    "    if study.best_params:\n",
    "        pipe.set_params(**study.best_params)\n",
    "    \n",
    "    best_score = study.best_value or 0.0\n",
    "    n_completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    \n",
    "    print(f\"âœ… {best_score:.4f} [{n_completed} trials]\")\n",
    "    \n",
    "    optimized_models.append((name, pipe))\n",
    "    results.append(best_score)\n",
    "\n",
    "# Fixed visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "names = [name for name, _ in optimized_models]\n",
    "bars = ax.bar(names, results, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_ylabel('Combined AML Score')\n",
    "ax.set_title('Optuna Model Performance (Multi-Metric Objective)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, results):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best performing model: {names[np.argmax(results)]} ({max(results):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Save all resulting models**\n",
    "\n",
    "Let's save all models\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to save models from training\n",
    "    folderDir = \"./models/mvp-kyt-sup-main-v2\"\n",
    "    os.makedirs(folderDir, exist_ok=True)\n",
    "    for name, pipe in optimized_models:\n",
    "        joblib.dump(pipe, f\"{folderDir}/{name}.pkl\", compress=True)\n",
    "    print(f\"ðŸ’¾ Saved {len(optimized_models)} models: {[name for name, _ in optimized_models]}\")\n",
    "except NameError:\n",
    "    # Load models if optimized_models doesn't exist\n",
    "    optimized_models = []\n",
    "    if os.path.exists(folderDir):\n",
    "        for file in os.listdir(folderDir):\n",
    "            if file.endswith('.pkl'):\n",
    "                name = file.replace('.pkl', '')\n",
    "                pipe = joblib.load(f\"{folderDir}/{file}\")\n",
    "                optimized_models.append((name, pipe))\n",
    "        print(f\"ðŸ“ Loaded {len(optimized_models)} models: {[name for name, _ in optimized_models]}\")\n",
    "    else:\n",
    "        print(\"âŒ No models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Validate all models and select the best models**\n",
    "\n",
    "Let's validate and select the best models\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best 3 models based on test accuracy\n",
    "test_results = []\n",
    "for name, pipe in optimized_models:\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_results.append((name, accuracy))\n",
    "    print(f\"{name}: Test Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Comparison Boxplot\n",
    "fig = plt.figure(figsize=(25,6))\n",
    "fig.suptitle('Models Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot([x[1] for x in test_results]) \n",
    "ax.set_xticklabels([x[0] for x in test_results])\n",
    "plt.show()\n",
    "\n",
    "# Sort by test accuracy and get top 3\n",
    "test_results.sort(key=lambda x: x[2], reverse=True)\n",
    "best_optimized_models = [(name, model) for name, model, acc in test_results[:3]]\n",
    "\n",
    "print(f\"\\nðŸ† Final top 3 models: {[name for name, _ in best_optimized_models]}\")\n",
    "print(f\"ðŸ“Š Test accuracies: {[f'{acc:.4f}' for _, _, acc in test_results[:3]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Use best models to predict unknown data**\n",
    "\n",
    "Let`s apply the model into unknown data\n",
    "\n",
    "[Description]\n",
    "[Whys]\n",
    "[What]\n",
    "[When]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best model to unlabeled data\n",
    "best_model = best_optimized_models[0][1]  # Champion model\n",
    "X_unlabeled = df_unlabeled.drop(['class', 'txId'], axis=1)\n",
    "\n",
    "predictions = best_model.predict(X_unlabeled)\n",
    "probabilities = best_model.predict_proba(X_unlabeled)[:, 1]\n",
    "display(predictions)\n",
    "print(f\"ðŸ”® Predictions on {len(X_unlabeled)} unlabeled transactions\")\n",
    "#print(f\"Illicit predictions: {sum(predictions)} ({sum(predictions)/len(predictions)*101:.1f}%)\")\n",
    "print(f\"Risk scores range: {probabilities.min():.3f} - {probabilities.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "[conclusion]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
