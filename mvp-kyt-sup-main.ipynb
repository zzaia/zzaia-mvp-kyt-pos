{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Know Your Transaction (KYT) - Transaction Risk Classification Engine\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook presents a comprehensive implementation of a Transaction Risk Classification Engine for Anti-Money Laundering (AML) compliance in cryptocurrency transactions. The project addresses the critical need for risk assessment of Bitcoin transactions by combining traditional AML indicators with blockchain-specific risk factors.\n",
    "\n",
    "### Domain Context: Financial AML for Transactions\n",
    "\n",
    "#### Core Domain Definition\n",
    "Anti-Money Laundering (AML) for transactions encompasses the comprehensive framework of laws, regulations, procedures, and technological solutions designed to prevent criminals from disguising illegally obtained funds as legitimate income through the global financial system. This domain includes detection, prevention, and reporting of money laundering, terrorist financing, tax evasion, market manipulation, and misuse of public funds.\n",
    "\n",
    "### Problem Definition: Transaction Risk Classification Engine\n",
    "\n",
    "#### Problem Statement\n",
    "Develop a system that assigns risk classifications to cryptocurrency transactions in real-time, integrating traditional AML indicators with blockchain-specific risk factors including wallet clustering, transaction graph analysis, and counterparty reputation scoring.\n",
    "\n",
    "#### Technical Requirements\n",
    "- **Problem Type**: Classification \n",
    "- **Processing Speed**: Sub-second analysis for high-frequency transactions\n",
    "- **Difficulty Level**: High - requires complex multi-dimensional data processing\n",
    "- **Output Format**: Risk binary classification (illicit = 1 /licit = 2)\n",
    "\n",
    "#### Data Landscape\n",
    "The system can processes multiple data dimensions:\n",
    "- Transaction metadata (amounts, timestamps, fees)\n",
    "- Wallet addresses and clustering information\n",
    "- Transaction graph relationships and network topology\n",
    "- Counterparty databases and reputation scores\n",
    "- Sanctions lists and regulatory databases\n",
    "- Temporal patterns and behavioral baselines\n",
    "\n",
    "### References\n",
    "\n",
    "This notebook implementation is based on the comprehensive research and analysis conducted during the project development phase. The following reference documents were used in the composition of this initial description:\n",
    "\n",
    "- **Domain Research**: [current-domain.md](domains/current-domain.md) - Contains detailed market analysis, regulatory framework research, and commercial viability assessment for the Financial AML domain;\n",
    "- **Problem Analysis**: [current-problem.md](problems/current-problem.md) - Provides comprehensive problem refinement, technical requirements analysis, and solution approach evaluation;\n",
    "- **Dataset Evaluation**: [current-dataset.md](datasets/current-dataset.md) - Documents dataset selection criteria, suitability scoring, and detailed feature analysis for the Elliptic dataset;\n",
    "- **Dataset Analysis & Preprocessing**: [dataset-analysis-and-preprocessing.ipynb](datasets/scripts/dataset-analysis-and-preprocessing.ipynb) - Comprehensive Jupyter notebook containing Elliptic dataset download, exploratory data analysis, and ML preparation steps;\n",
    "\n",
    "These reference documents contain the foundational research that informed the technical approach, feature engineering strategy, and implementation decisions reflected in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the primary entry point for the MVP KYT implementation and it can run independently, providing both technical implementation and business context for real cryptocurrency transaction risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Comprehensive installation and import of all required libraries for machine learning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (4.1.0)\n",
      "Requirement already satisfied: scikeras in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (0.13.0)\n",
      "Requirement already satisfied: tensorflow in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pytorch-tabnet) (2.2.6)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pytorch-tabnet) (1.7.1)\n",
      "Requirement already satisfied: scipy>1.4 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pytorch-tabnet) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.3 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pytorch-tabnet) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from scikeras) (3.11.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from keras>=3.2.0->scikeras) (14.1.0)\n",
      "Requirement already satisfied: namex in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from torch>=1.3->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (3.1.1)\n",
      "Requirement already satisfied: lightgbm in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (4.6.0)\n",
      "Requirement already satisfied: catboost in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (1.2.8)\n",
      "Requirement already satisfied: numpy in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from xgboost) (2.27.5)\n",
      "Requirement already satisfied: scipy in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from xgboost) (1.16.0)\n",
      "Requirement already satisfied: graphviz in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from catboost) (3.10.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from catboost) (2.3.2)\n",
      "Requirement already satisfied: plotly in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from catboost) (6.3.0)\n",
      "Requirement already satisfied: six in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from plotly->catboost) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azure-storage-blob in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (12.26.0)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-storage-blob) (1.35.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-storage-blob) (46.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-storage-blob) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-storage-blob) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from cryptography>=2.1.4->azure-storage-blob) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 08:09:46.854855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-tabnet scikeras tensorflow\n",
    "%pip install xgboost lightgbm catboost\n",
    "%pip install azure-storage-blob\n",
    "\n",
    "#Colab setup only\n",
    "#!git clone https://github.com/zzaia/zzaia-mvp-kyt-pos.git\n",
    "#%cd zzaia-mvp-kyt-pos\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "\n",
    "# Add datasets/scripts to Python path\n",
    "scripts_path = Path(\"./datasets/scripts\")\n",
    "if str(scripts_path.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_path.resolve()))\n",
    "\n",
    "# Add models/scripts to Python path\n",
    "models_scripts_path = Path(\"./models/scripts\")\n",
    "if str(models_scripts_path.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(models_scripts_path.resolve()))\n",
    "\n",
    "# Import Azure utilities\n",
    "from azure_utils import AzureBlobDownloader\n",
    "azureClient = AzureBlobDownloader(\"https://stmvppos.blob.core.windows.net\", \"mvpkytsup\")\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-processed datasets\n",
    "\n",
    "Let's load the pre-processed and compressed data from remote and local sources. The dataset is a composition of cryptocurrency transactions in the Bitcoin blockchain. \n",
    "\n",
    "It has 166 features, of which 92 features represent local transaction information and another 72 features represent aggregated information from one-hop neighboring transactions (directly linked transactions). Thus, the dataset already has curated information about the relationships between transactions. \n",
    "\n",
    "This is very important because money laundering and fraud patterns often involve coordinated transaction clusters and neighborhood patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from HDF5: (203769, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (46564, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (157205, 168) - All subsequent operations will use compressed data\n",
      "Loaded from HDF5: (234355, 2) - All subsequent operations will use compressed data\n",
      "\n",
      "üìä Dataset Summary:\n",
      "  - Features: 203,769 transactions √ó 166 features\n",
      "  - Labeled: 46,564 transactions\n",
      "  - Unlabeled: 157,205 transactions\n",
      "  - Edges: 234,355 transaction relationships\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "dataset_dir = Path(\"elliptic_bitcoin_dataset\")\n",
    "root_dir = Path(\"./datasets\")\n",
    "processed_dir = root_dir / \"processed\" / dataset_dir\n",
    "root_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download processed data from Azure if not present locally\n",
    "if not processed_dir.exists() or not any(processed_dir.iterdir()):\n",
    "    print(f\"Local processed directory is empty. Downloading from Azure...\")\n",
    "    azureClient.download_documents(\"datasets/processed\", dataset_dir.name, base_path=\"./\")\n",
    "\n",
    "# The complete dataset already pre-processed \n",
    "df_complete = pd.read_hdf(processed_dir / \"df_complete.h5\", key=\"df_complete\")\n",
    "print(f\"Loaded from HDF5: {df_complete.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered labeled dataset already pre-processed\n",
    "df_labeled = pd.read_hdf(processed_dir / \"df_labeled.h5\", key=\"df_labeled\")\n",
    "print(f\"Loaded from HDF5: {df_labeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The filtered unlabeled dataset already pre-processed\n",
    "df_unlabeled = pd.read_hdf(processed_dir / \"df_unlabeled.h5\", key=\"df_unlabeled\")\n",
    "print(f\"Loaded from HDF5: {df_unlabeled.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# The edges dataset that maps relationships between transaction nodes\n",
    "df_edges = pd.read_hdf(processed_dir / \"df_edges.h5\", key=\"df_edges\")\n",
    "print(f\"Loaded from HDF5: {df_edges.shape} - All subsequent operations will use compressed data\")\n",
    "\n",
    "# Summary of all datasets\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  - Features: {df_complete.shape[0]:,} transactions √ó {df_complete.shape[1] -2} features\")\n",
    "print(f\"  - Labeled: {df_labeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Unlabeled: {df_unlabeled.shape[0]:,} transactions\")\n",
    "print(f\"  - Edges: {df_edges.shape[0]:,} transaction relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Strategy\n",
    "\n",
    "Let's apply machine learning techniques to the labeled dataset portion using supervised learning, and apply the prediction to the unknown unlabeled dataset portion in order to establish a performance baseline for future improvements.\n",
    "\n",
    "Following these steps:\n",
    "\n",
    "1. Define overall parameters and make data splits;\n",
    "2. Define all training models to be used;\n",
    "3. Define all pipelines to be used during training;\n",
    "4. Define model parameter distributions for a grid search approach;\n",
    "5. Define the score function to be used during training;\n",
    "6. Execute the training;\n",
    "7. Save all resulting models;\n",
    "8. Validate all models and select the best models;\n",
    "9. Use best models to predict unknown data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Define overall parameters and make data splits**\n",
    "\n",
    "After splitting the labeled dataset into training and validation (test) sets, let's prepare the training dataset for training and validation using the stratified approach, which generates a fixed number of splits (folds) for the dataset following a fixed proportion of train/test samples. \n",
    "\n",
    "The main idea of this approach is to guarantee training without bias toward a specific dataset split because it maintains the same class proportion for each split (fold) generated. We use this technique in labeled and supervised learning, assuming that the dataset's pattern does not have significant changes over time.\n",
    "\n",
    "In this specific dataset, we consider the timestamp as a grouping factor for transactions but not as a changing factor for the dataset's pattern over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining overall parameters\n",
    "random_seed = 4354 # PARAMETER: random seed\n",
    "test_size_split = 0.20 # PARAMETER: test set size\n",
    "n_stratified_splits = 2 # PARAMETER: number of folds\n",
    "n_pca_components = 0.95 # PARAMETER: PCA components to keep\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Prepare data\n",
    "x_labeled = df_labeled.drop(['class', 'txId'], axis=1)\n",
    "y_labeled = df_labeled['class']\n",
    "\n",
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_labeled, y_labeled,\n",
    "    test_size=test_size_split, \n",
    "    shuffle=True, \n",
    "    random_state=random_seed, \n",
    "    stratify=y_labeled) # stratified holdout\n",
    "\n",
    "# Cross-validation setup to be applied in the training set \n",
    "cv = StratifiedKFold(n_splits=n_stratified_splits, \n",
    "                     shuffle=True, \n",
    "                     random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Define all models and pipelines to be used during training**\n",
    "\n",
    "Let's define which ML pipelines to use during training, by configuring which pre-processing operations and models will be used in the training process. Pipelines also help avoid data leakage by ensuring that feature transformation is applied only to the training dataset portion.          \n",
    "\n",
    "For feature transformation, the standard scaler was used to ensure the best scale for feature values. By normalizing all values to a common metric, it reduces bias toward feature magnitude and enables subsequent operations to capture important patterns between features without being influenced mainly by their magnitude. All features contribute equally to pattern detection. This is especially important in finance because the difference between feature scales can be significant.\n",
    "\n",
    "For feature dimensionality reduction, Principal Component Analysis (PCA) was used to reduce from 166 to only 59 features. We aim to reduce feature dimensionality while maximizing the dissimilarity of the original dataset, thus extracting the most discriminative features and improving model training performance. It must be applied after the standard scaler to avoid the mentioned magnitude bias, and should be used in datasets that have a large quantity of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loaded 20 pipeline wrappers\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline wrappers\n",
    "from lr_wrapper import LRWrapper\n",
    "from knn_wrapper import KNNWrapper\n",
    "from cart_wrapper import CARTWrapper\n",
    "from nb_wrapper import NBWrapper\n",
    "from svm_wrapper import SVMWrapper\n",
    "from bagging_wrapper import BaggingWrapper\n",
    "from voting_soft_wrapper import VotingSoftWrapper\n",
    "from rf_wrapper import RFWrapper\n",
    "from et_wrapper import ETWrapper\n",
    "from ada_wrapper import AdaWrapper\n",
    "from gb_wrapper import GBWrapper\n",
    "from stacking_wrapper import StackingWrapper\n",
    "from stacking_adv_wrapper import StackingAdvWrapper\n",
    "from bag_knn_wrapper import BagKNNWrapper\n",
    "from xgboost_wrapper import XGBoostWrapper\n",
    "from lightgbm_wrapper import LightGBMWrapper\n",
    "from catboost_wrapper import CatBoostWrapper\n",
    "from histgb_wrapper import HistGBWrapper\n",
    "from tabnet_wrapper import TabNetWrapper\n",
    "from fnn_wrapper import FNNWrapper\n",
    "\n",
    "# Create pipeline wrapper instances\n",
    "pipeline_wrappers = []\n",
    "# Uncomment models as needed for training\n",
    "pipeline_wrappers.append(LRWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(NBWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(KNNWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(CARTWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(SVMWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(BaggingWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(VotingSoftWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(RFWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(ETWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(AdaWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(GBWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(StackingWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(StackingAdvWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(BagKNNWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(XGBoostWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(LightGBMWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(CatBoostWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(HistGBWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(TabNetWrapper(random_seed=random_seed))\n",
    "pipeline_wrappers.append(FNNWrapper(random_seed=random_seed))\n",
    "\n",
    "print(f\"üì¶ Loaded {len(pipeline_wrappers)} pipeline wrappers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define model parameter distributions for a grid search approach**\n",
    "\n",
    "Let's prepare the parameter distributions for a random grid search, by using a distribution of possible parameter values, so that the training phase can explore the best performance models also from the perspective of their hyperparameters. This is a better solution than the common grid search approach because it can explore a broader hyperparameter space and is often faster. \n",
    "\n",
    "Three types of parameter distributions were used:\n",
    "\n",
    "- uniform: Continuous values with equal probability across a range, used when all values in the range are equally valid;\n",
    "- loguniform: Continuous values on logarithmic scale (exponential distribution), used when smaller values are often better;\n",
    "- randint: Discrete integer values with equal probability, used for discrete hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Parameter distributions loaded for 20 models\n"
     ]
    }
   ],
   "source": [
    "# Build parameter distributions from wrappers\n",
    "param_distributions = {}\n",
    "for wrapper in pipeline_wrappers:\n",
    "    param_distributions[wrapper.name] = wrapper.get_param_distributions()\n",
    "\n",
    "print(f\"üìä Parameter distributions loaded for {len(param_distributions)} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define the score function and the objective function to be used during training**\n",
    "\n",
    "Let's define the score function that will be used to measure each model's performance. Instead of using just one metric alone, the function enables us to define a weighted multi-metric approach, defining which metrics would be more important for the model's performance. The chosen score is a combination of three important metrics: \n",
    "\n",
    "- Recall measures how good the model is at not having false negatives;\n",
    "- Precision measures how good the model is at not having false positives;  \n",
    "- Accuracy measures how good the model is at not having false classifications;\n",
    "\n",
    "In financial transaction risk assessments, it is more important to have fewer false negatives than false positive classifications, because it would be less risky to block a transaction wrongly considered illicit than to not block a transaction wrongly considered licit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä AML Scoring Metric:\n",
      "----------------------------------------------------------------------\n",
      "AML Score = 0.3 √ó MCC + 0.2 √ó Cost Score + 0.5 √ó PR-AUC\n",
      "\n",
      "Components:\n",
      "  MCC (Matthews Correlation Coefficient):\n",
      "    - Threshold-dependent: Evaluates classification quality at specific threshold\n",
      "    - Measures: Correlation between predictions and actuals using all 4 confusion matrix values\n",
      "    - Range: -1 (worst) to +1 (perfect), 0 = random\n",
      "    - Key strength: Balanced metric, reliable for imbalanced data\n",
      "\n",
      "  Cost Score:\n",
      "    - Threshold-dependent: Business impact at specific threshold\n",
      "    - Measures: Financial cost of classification errors\n",
      "    - Formula: 1 - Total Cost / Max Cost\n",
      "    - Total Cost = TN √ó 0 + TP √ó 0 + FP √ó 1 + FN √ó 10\n",
      "    - Key strength: Incorporates asymmetric business costs (FN >> FP)\n",
      "\n",
      "  PR-AUC (Precision-Recall Area Under Curve):\n",
      "    - Threshold-independent: Evaluates performance across ALL thresholds\n",
      "    - Measures: Model's discriminative ability and probability calibration quality\n",
      "    - Range: 0 to 1 (1 = perfect)\n",
      "    - Key strength: Ensures robustness if threshold needs adjustment\n",
      "\n",
      "Key Takeaways:\n",
      "  ‚Ä¢ MCC: 'How good is this specific prediction?' (threshold-dependent quality)\n",
      "  ‚Ä¢ Cost Score: 'What is the business impact?' (domain-specific evaluation)\n",
      "  ‚Ä¢ PR-AUC: 'How good is this model overall?' (threshold-independent capability)\n",
      "  ‚Ä¢ Together: Comprehensive evaluation of classification quality, business value, and model robustness\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN = True Negatives (correctly identified licit)\n",
      "  TP = True Positives (correctly identified illicit)\n",
      "  FP = False Positives (licit flagged as illicit)\n",
      "  FN = False Negatives (illicit missed)\n",
      "  N = Total samples\n",
      "  Max Cost = N √ó 10 (worst case: all FN)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from aml_scorer import AMLScorer\n",
    "\n",
    "# Instantiate scorer\n",
    "aml_scorer = AMLScorer(cost_fp=1, cost_fn=10, cost_tn=0, cost_tp=0, mcc_weight=0.3, cost_weight=0.2, prauc_weight=0.5)\n",
    "\n",
    "# Create sklearn scorer\n",
    "composite_scorer = make_scorer(aml_scorer.score)\n",
    "\n",
    "# Print metric equation\n",
    "print(\"üìä AML Scoring Metric:\")\n",
    "print(\"-\" * 70)\n",
    "print(aml_scorer.metric_equation)\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Execute the training** [CAN BE SKIPPED > 2h]\n",
    "\n",
    "Let's execute the training phase using random grid search and execute it in parallel, with cross-validation of all dataset splits (folds) and rank the best models by score function.\n",
    "\n",
    "The final plot will display all model training samples with their mean and variance performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Training 20 models (patience=1, timeout=2.0h)\n",
      "Checkpoints: models/mvp-kyt-sup-main\n",
      "------------------------------------------------------------\n",
      "Loading checkpoint LR... ‚úÖ 0.8981 (¬±0.0046) [2/2] is timed out: False]\n",
      "Loading checkpoint NB... ‚úÖ 0.7267 (¬±0.0042) [2/2] is timed out: False]\n",
      "Loading checkpoint KNN... ‚úÖ 0.9335 (¬±0.0009) [2/2] is timed out: False]\n",
      "Loading checkpoint CART... ‚úÖ 0.9189 (¬±0.0040) [2/2] is timed out: False]\n",
      "Loading checkpoint SVM... ‚úÖ 0.9360 (¬±0.0011) [2/2] is timed out: False]\n",
      "Loading checkpoint Bag... ‚úÖ 0.9401 (¬±0.0026) [2/2] is timed out: False]\n",
      "Loading checkpoint Vote-Soft... ‚úÖ 0.9384 (¬±0.0010) [2/2] is timed out: False]\n",
      "Loading checkpoint RF... ‚úÖ 0.9242 (¬±0.0001) [2/2] is timed out: False]\n",
      "Loading checkpoint ET... ‚úÖ 0.6956 (¬±0.0001) [2/2] is timed out: False]\n",
      "Loading checkpoint Ada... ‚úÖ 0.6941 (¬±0.0004) [2/2] is timed out: False]\n",
      "Loading checkpoint GB... ‚úÖ 0.9475 (¬±0.0008) [2/2] is timed out: False]\n",
      "Training Stack... ‚úÖ 0.9259 (¬±0.0003) [2/2] is timed out: False]\n",
      "Training Stack-Adv... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-03 08:21:05,224] Trial 0 failed with parameters: {'pca__n_components': 0.917774245222722, 'pca__whiten': True, 'pca__svd_solver': 'auto', 'stacking__fnn__model__hidden_dims': [128, 64], 'stacking__fnn__model__learning_rate': 0.0006388856720487872, 'stacking__fnn__model__dropout': 0.25285313843737084, 'stacking__fnn__epochs': 30, 'stacking__xgb__n_estimators': 377, 'stacking__xgb__max_depth': 3, 'stacking__xgb__learning_rate': 0.034988887251761656, 'stacking__xgb__subsample': 0.7424534992104972, 'stacking__xgb__colsample_bytree': 0.7519409592840439, 'stacking__xgb__scale_pos_weight': 11.42902194226668, 'stacking__tabnet__n_d': 16, 'stacking__tabnet__n_a': 8, 'stacking__tabnet__n_steps': 4, 'stacking__tabnet__gamma': 1.1104674292062413, 'stacking__final_estimator__C': 0.03081937519520943, 'stacking__final_estimator__solver': 'lbfgs', 'threshold': 0.6279949072355162} because of the following error: ValueError(\"Invalid parameter 'threshold' for estimator Pipeline(steps=[('std', StandardScaler()), ('pca', PCA(n_components=0.95)),\\n                ('stacking',\\n                 StackingClassifier(cv=3,\\n                                    estimators=[('fnn',\\n                                                 KerasClassifier(batch_size=128, epochs=50, model=<function StackingAdvWrapper.create_fnn at 0x72c76fc35260>, random_state=4354, verbose=0)),\\n                                                ('xgb',\\n                                                 XGBClassifier(base_score=None,\\n                                                               booster=None,\\n                                                               callbacks=None,\\n                                                               colsample_bylevel=None,\\n                                                               co...\\n                                                                  lambda_sparse=0.001,\\n                                                                  seed=4354,\\n                                                                  clip_value=1,\\n                                                                  verbose=0,\\n                                                                  optimizer_fn=<class 'torch.optim.adam.Adam'>,\\n                                                                  optimizer_params={'lr': 0.02},\\n                                                                  scheduler_fn=None,\\n                                                                  scheduler_params={},\\n                                                                  mask_type='sparsemax',\\n                                                                  input_dim=None,\\n                                                                  output_dim=None,\\n                                                                  device_name='auto',\\n                                                                  n_shared_decoder=1,\\n                                                                  n_indep_decoder=1,\\n                                                                  grouped_features=[]))],\\n                                    final_estimator=LogisticRegression(max_iter=2000,\\n                                                                       random_state=4354)))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/raphael-pizzaia/zzaia/zzaia-main-workspace/workspace/zzaia-mvp-kyt-pos.worktrees/improvements/add-more-advanced-models/models/scripts/aml_scorer.py\", line 177, in objective\n",
      "    pipeline.set_params(**params)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py\", line 319, in set_params\n",
      "    self._set_params(\"steps\", **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/metaestimators.py\", line 69, in _set_params\n",
      "    super().set_params(**params)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"/home/raphael-pizzaia/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py\", line 345, in set_params\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Invalid parameter 'threshold' for estimator Pipeline(steps=[('std', StandardScaler()), ('pca', PCA(n_components=0.95)),\n",
      "                ('stacking',\n",
      "                 StackingClassifier(cv=3,\n",
      "                                    estimators=[('fnn',\n",
      "                                                 KerasClassifier(batch_size=128, epochs=50, model=<function StackingAdvWrapper.create_fnn at 0x72c76fc35260>, random_state=4354, verbose=0)),\n",
      "                                                ('xgb',\n",
      "                                                 XGBClassifier(base_score=None,\n",
      "                                                               booster=None,\n",
      "                                                               callbacks=None,\n",
      "                                                               colsample_bylevel=None,\n",
      "                                                               co...\n",
      "                                                                  lambda_sparse=0.001,\n",
      "                                                                  seed=4354,\n",
      "                                                                  clip_value=1,\n",
      "                                                                  verbose=0,\n",
      "                                                                  optimizer_fn=<class 'torch.optim.adam.Adam'>,\n",
      "                                                                  optimizer_params={'lr': 0.02},\n",
      "                                                                  scheduler_fn=None,\n",
      "                                                                  scheduler_params={},\n",
      "                                                                  mask_type='sparsemax',\n",
      "                                                                  input_dim=None,\n",
      "                                                                  output_dim=None,\n",
      "                                                                  device_name='auto',\n",
      "                                                                  n_shared_decoder=1,\n",
      "                                                                  n_indep_decoder=1,\n",
      "                                                                  grouped_features=[]))],\n",
      "                                    final_estimator=LogisticRegression(max_iter=2000,\n",
      "                                                                       random_state=4354)))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].\n",
      "[W 2025-11-03 08:21:05,226] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'threshold' for estimator Pipeline(steps=[('std', StandardScaler()), ('pca', PCA(n_components=0.95)),\n                ('stacking',\n                 StackingClassifier(cv=3,\n                                    estimators=[('fnn',\n                                                 KerasClassifier(batch_size=128, epochs=50, model=<function StackingAdvWrapper.create_fnn at 0x72c76fc35260>, random_state=4354, verbose=0)),\n                                                ('xgb',\n                                                 XGBClassifier(base_score=None,\n                                                               booster=None,\n                                                               callbacks=None,\n                                                               colsample_bylevel=None,\n                                                               co...\n                                                                  lambda_sparse=0.001,\n                                                                  seed=4354,\n                                                                  clip_value=1,\n                                                                  verbose=0,\n                                                                  optimizer_fn=<class 'torch.optim.adam.Adam'>,\n                                                                  optimizer_params={'lr': 0.02},\n                                                                  scheduler_fn=None,\n                                                                  scheduler_params={},\n                                                                  mask_type='sparsemax',\n                                                                  input_dim=None,\n                                                                  output_dim=None,\n                                                                  device_name='auto',\n                                                                  n_shared_decoder=1,\n                                                                  n_indep_decoder=1,\n                                                                  grouped_features=[]))],\n                                    final_estimator=LogisticRegression(max_iter=2000,\n                                                                       random_state=4354)))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m training_manager = TrainingManager(\n\u001b[32m     13\u001b[39m     checkpoint_dir=checkpoint_dir,\n\u001b[32m     14\u001b[39m     n_trials=n_trials,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     random_seed=random_seed\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m trained_models = training_manager.train_models(\n\u001b[32m     23\u001b[39m     pipeline_wrappers=pipeline_wrappers,\n\u001b[32m     24\u001b[39m     param_distributions=param_distributions,\n\u001b[32m     25\u001b[39m     X_train=X_train,\n\u001b[32m     26\u001b[39m     y_train=y_train,\n\u001b[32m     27\u001b[39m     cv=cv,\n\u001b[32m     28\u001b[39m     scorer=composite_scorer,\n\u001b[32m     29\u001b[39m     aml_scorer=aml_scorer,\n\u001b[32m     30\u001b[39m     n_pca_components=n_pca_components,\n\u001b[32m     31\u001b[39m     azure_client=azureClient\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[32m     35\u001b[39m fig = plt.figure(figsize=(\u001b[32m25\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/zzaia/zzaia-main-workspace/workspace/zzaia-mvp-kyt-pos.worktrees/improvements/add-more-advanced-models/models/scripts/training_manager.py:330\u001b[39m, in \u001b[36mTrainingManager.train_models\u001b[39m\u001b[34m(self, pipeline_wrappers, param_distributions, X_train, y_train, cv, scorer, aml_scorer, n_pca_components, azure_client)\u001b[39m\n\u001b[32m    327\u001b[39m early_stopping.start_timer()\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m study.optimize(objective, n_trials=\u001b[38;5;28mself\u001b[39m.n_trials, callbacks=[early_stopping])\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n\u001b[32m    333\u001b[39m pipeline_params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    334\u001b[39m study.best_params.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m'\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m'\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     _optimize(\n\u001b[32m    491\u001b[39m         study=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    492\u001b[39m         func=func,\n\u001b[32m    493\u001b[39m         n_trials=n_trials,\n\u001b[32m    494\u001b[39m         timeout=timeout,\n\u001b[32m    495\u001b[39m         n_jobs=n_jobs,\n\u001b[32m    496\u001b[39m         catch=\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[32m    497\u001b[39m         callbacks=callbacks,\n\u001b[32m    498\u001b[39m         gc_after_trial=gc_after_trial,\n\u001b[32m    499\u001b[39m         show_progress_bar=show_progress_bar,\n\u001b[32m    500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         _optimize_sequential(\n\u001b[32m     64\u001b[39m             study,\n\u001b[32m     65\u001b[39m             func,\n\u001b[32m     66\u001b[39m             n_trials,\n\u001b[32m     67\u001b[39m             timeout,\n\u001b[32m     68\u001b[39m             catch,\n\u001b[32m     69\u001b[39m             callbacks,\n\u001b[32m     70\u001b[39m             gc_after_trial,\n\u001b[32m     71\u001b[39m             reseed_sampler_rng=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m             time_start=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m             progress_bar=progress_bar,\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = _run_trial(study, func, catch)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = func(trial)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/zzaia/zzaia-main-workspace/workspace/zzaia-mvp-kyt-pos.worktrees/improvements/add-more-advanced-models/models/scripts/aml_scorer.py:177\u001b[39m, in \u001b[36mAMLScorer.create_objective.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    174\u001b[39m     threshold = trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.9\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Set pipeline parameters (exclude threshold as it's not a pipeline param)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m pipeline.set_params(**params)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Perform custom cross-validation with threshold\u001b[39;00m\n\u001b[32m    180\u001b[39m scores = \u001b[38;5;28mself\u001b[39m.cross_val_score_with_threshold(pipeline, X_train, y_train, cv, threshold)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/pipeline.py:319\u001b[39m, in \u001b[36mPipeline.set_params\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m    303\u001b[39m \u001b[33;03m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m        Pipeline class instance.\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_params(\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m, **kwargs)\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/utils/metaestimators.py:69\u001b[39m, in \u001b[36m_BaseComposition._set_params\u001b[39m\u001b[34m(self, attr, **params)\u001b[39m\n\u001b[32m     66\u001b[39m                 \u001b[38;5;28mself\u001b[39m._replace_estimator(attr, name, params.pop(name))\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28msuper\u001b[39m().set_params(**params)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/sklearn/base.py:345\u001b[39m, in \u001b[36mBaseEstimator.set_params\u001b[39m\u001b[34m(self, **params)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[32m    344\u001b[39m     local_valid_params = \u001b[38;5;28mself\u001b[39m._get_param_names()\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    346\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m     )\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[32m    351\u001b[39m     nested_params[key][sub_key] = value\n",
      "\u001b[31mValueError\u001b[39m: Invalid parameter 'threshold' for estimator Pipeline(steps=[('std', StandardScaler()), ('pca', PCA(n_components=0.95)),\n                ('stacking',\n                 StackingClassifier(cv=3,\n                                    estimators=[('fnn',\n                                                 KerasClassifier(batch_size=128, epochs=50, model=<function StackingAdvWrapper.create_fnn at 0x72c76fc35260>, random_state=4354, verbose=0)),\n                                                ('xgb',\n                                                 XGBClassifier(base_score=None,\n                                                               booster=None,\n                                                               callbacks=None,\n                                                               colsample_bylevel=None,\n                                                               co...\n                                                                  lambda_sparse=0.001,\n                                                                  seed=4354,\n                                                                  clip_value=1,\n                                                                  verbose=0,\n                                                                  optimizer_fn=<class 'torch.optim.adam.Adam'>,\n                                                                  optimizer_params={'lr': 0.02},\n                                                                  scheduler_fn=None,\n                                                                  scheduler_params={},\n                                                                  mask_type='sparsemax',\n                                                                  input_dim=None,\n                                                                  output_dim=None,\n                                                                  device_name='auto',\n                                                                  n_shared_decoder=1,\n                                                                  n_indep_decoder=1,\n                                                                  grouped_features=[]))],\n                                    final_estimator=LogisticRegression(max_iter=2000,\n                                                                       random_state=4354)))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose']."
     ]
    }
   ],
   "source": [
    "# Import training manager\n",
    "from training_manager import TrainingManager\n",
    "\n",
    "# Training parameters\n",
    "n_trials = 2 # PARAMETER: number of hyperparameter trials per model\n",
    "n_jobs = 1 \n",
    "patience_ratio = 0.5 # PARAMETER: early stopping patience ratio\n",
    "timeout_seconds = 2 * 60 * 60 # PARAMETER: 2 hours \n",
    "checkpoint_dir = Path(\"./models/mvp-kyt-sup-main\")\n",
    "\n",
    "# Create training manager\n",
    "training_manager = TrainingManager(\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    n_trials=n_trials,\n",
    "    patience_ratio=patience_ratio,\n",
    "    timeout_seconds=timeout_seconds,\n",
    "    n_jobs=n_jobs,\n",
    "    random_seed=random_seed\n",
    ")\n",
    "\n",
    "# Train models\n",
    "trained_models = training_manager.train_models(\n",
    "    pipeline_wrappers=pipeline_wrappers,\n",
    "    param_distributions=param_distributions,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    cv=cv,\n",
    "    scorer=composite_scorer,\n",
    "    aml_scorer=aml_scorer,\n",
    "    n_pca_components=n_pca_components,\n",
    "    azure_client=azureClient\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(25,6))\n",
    "fig.suptitle('Model Comparison - CV Score Distribution') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot([cv_scores for _, cv_scores, _, _, _ in trained_models], labels=[name for name, _, _, _, _ in trained_models])\n",
    "ax.set_ylabel('CV AML Score')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Load all resulting models**\n",
    "\n",
    "Retrieve previously trained models from local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'training_models' in locals():\n",
    "    trained_models = training_manager.load_models_from_checkpoint(\n",
    "        azure_client=azureClient\n",
    "    )\n",
    "    print(f\"üíæ {len(trained_models)} models in {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Validate all models and select the best models**\n",
    "\n",
    "Let's validate and select the best models by applying all trained pipelines to the previously generated testing set using the multi-metric score function.    \n",
    "\n",
    "Model validation with an unseen dataset during training can give us an approximate measure of how the model would perform in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best models based on composite score with optimized thresholds\n",
    "test_results = []\n",
    "for name, cv_scores, pipe, study, threshold in trained_models:\n",
    "    # Generate predictions\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]  # Probability of illicit class\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "    # Calculate PR-AUC (threshold-independent)\n",
    "    prauc = average_precision_score(y_test, y_proba)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    composite_score = aml_scorer.score(y_test, y_pred, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    test_results.append({\n",
    "        'name': name,\n",
    "        'threshold': threshold,\n",
    "        'composite': composite_score,\n",
    "        'mcc': mcc,\n",
    "        'prauc': prauc,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp\n",
    "    })\n",
    "\n",
    "# Sort by composite score\n",
    "test_results.sort(key=lambda x: x['composite'], reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüèÜ Final Model Rankings:\")\n",
    "print('-'*95)\n",
    "print(f\"{'Model':<12} {'Threshold':<11} {'Composite':<11} {'MCC':<8} {'PR-AUC':<8} {'TN':<6} {'FP':<5} {'FN':<5} {'TP':<5}\")\n",
    "print('-'*95)\n",
    "for result in test_results:\n",
    "    threshold_str = 'N/A' if result['threshold'] is None else f\"{result['threshold']:.3f}\"\n",
    "    print(f\"{result['name']:<12} {threshold_str:<11} {result['composite']:>7.4f}    \"\n",
    "          f\"{result['mcc']:>6.3f}  {result['prauc']:>6.4f}  \"\n",
    "          f\"{result['tn']:<6} {result['fp']:<5} {result['fn']:<5} {result['tp']:<5}\")\n",
    "\n",
    "# Store best model for downstream use\n",
    "best_model_name = test_results[0]['name']\n",
    "best_model = next(pipe for name, _, pipe, _, _ in trained_models if name == best_model_name)\n",
    "optimal_threshold = test_results[0]['threshold'] if test_results[0]['threshold'] is not None else 0.5\n",
    "print(f\"\\n‚úÖ Best model: {best_model_name} (threshold={optimal_threshold:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Optuna Optimization Analysis**\n",
    "\n",
    "Visualize Optuna optimization history and parameter importance for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Optuna Optimization Analysis for best model\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Extract study for best model\n",
    "best_study = next(study for name, _, _, study, _ in trained_models if name == best_model_name)\n",
    "\n",
    "print(f\"\\n{best_model_name} - Best Score: {best_study.best_value:.4f}\")\n",
    "print(f\"Best Parameters: {best_study.best_params}\")\n",
    "\n",
    "# Plot optimization history\n",
    "fig = optuna.visualization.plot_optimization_history(best_study)\n",
    "fig.update_layout(title=f\"{best_model_name} - Optimization History\")\n",
    "fig.show()\n",
    "    \n",
    "# Plot parameter importances\n",
    "try:\n",
    "    fig = optuna.visualization.plot_param_importances(best_study)\n",
    "    fig.update_layout(title=f\"{best_model_name} - Parameter Importances\")\n",
    "    fig.show()\n",
    "except:\n",
    "    print(f\"  (Not enough trials for parameter importance analysis)\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Use best models to predict unknown data**\n",
    "\n",
    "Let's apply the best pipeline model to unknown data‚Äîdata that does not have labels‚Äîand display the results to get an idea of how the landscape of unknown illicit transactions could be. This measure can also be used for comparison with future improvements to the machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best model to unlabeled data\n",
    "X_unlabeled = df_unlabeled.drop(['class', 'txId'], axis=1)\n",
    "y_proba = best_model.predict_proba(X_unlabeled)[:, 1]\n",
    "predictions = (y_proba >= optimal_threshold).astype(int)\n",
    "df_prediction = pd.Series(predictions, name=\"prediction\")\n",
    "df_final = pd.concat([df_unlabeled[['txId']], df_prediction], axis=1)\n",
    "df_final = df_final.applymap(lambda x: 'Illicit' if x == 0 else 'Licit' if x == 1 else x)\n",
    "\n",
    "# Analyze prediction distribution\n",
    "class_counts = df_final['prediction'].value_counts()\n",
    "labeled_only = class_counts[class_counts.index != 'unknown']\n",
    "imbalance_ratio = labeled_only.max() / labeled_only.min() if len(labeled_only) >= 2 else 1.0\n",
    "\n",
    "# Plot distribution\n",
    "print(f\"\\nüìà Prediction Distribution:\")\n",
    "print(f\"Model used:\", best_model_name)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['lightblue', 'orange', 'lightcoral'])\n",
    "ax1.set_title('Class Counts')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "class_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['lightblue', 'orange', 'lightcoral'])\n",
    "ax2.set_title('Class Distribution')\n",
    "ax2.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display prediction samples and summary statistics\n",
    "print(f\"üìä Prediction Summary:\")\n",
    "print(f\"  Total predictions: {len(df_final):,}\")\n",
    "print(f\"  Illicit transactions: {sum(df_final['prediction'] == 'Illicit'):,}\")\n",
    "print(f\"  Licit transactions: {sum(df_final['prediction'] == 'Licit'):,}\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Get sample transactions for analysis\n",
    "illicit_selector = df_final['prediction'] == 'Illicit'\n",
    "X_unlabeled_illicit = df_final[illicit_selector].head(100)\n",
    "print(f\"\\nüîÆ Sample illicit transactions (showing {len(X_unlabeled_illicit)} of {sum(illicit_selector):,} total)\")\n",
    "display(X_unlabeled_illicit)\n",
    "\n",
    "licit_selector = df_final['prediction'] == 'Licit'\n",
    "X_unlabeled_licit = df_final[licit_selector].head(100)\n",
    "print(f\"\\nüîÆ Sample licit transactions (showing {len(X_unlabeled_licit)} of {sum(licit_selector):,} total)\")\n",
    "display(X_unlabeled_licit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "\n",
    "Some considerations must be made before the conclusion:\n",
    "\n",
    "- Compressed data can increase training time but reduces dataset sizes without influencing the final result.\n",
    "\n",
    "- Models like Naive Bayes reached almost 0.6 in contrast to other models and were removed from training;  \n",
    "\n",
    "- Models like Logistic Regression also had poor performance and were removed from training, but they are used in some ensembles as default estimator models. \n",
    "\n",
    "**Future improvements to training:**\n",
    "\n",
    "- The unsupervised approach could produce good or even better results because it would use much more data to identify patterns by using the complete dataset. Also, the labeled dataset could indicate which clusters could be labeled with the illicit class;\n",
    "\n",
    "- The training could use models more recommended for graph-type datasets, such as Graph Convolutional Networks (GCN), making use of the edge dataset to learn patterns with deeper transaction chains, not only direct neighbors.\n",
    "\n",
    "**Production readiness:**\n",
    "\n",
    "- This training was performed on a dataset curated for research purposes. There is no information about which features were used, so in order to have a production-ready model, a new dataset in the same format would need to be gathered and curated;\n",
    "\n",
    "- A final performance indicator would need to be established to consider the model ready for a production environment, by classifying real labeled current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "#### Model Performance Summary\n",
    "\n",
    "This supervised learning approach successfully developed a high-performance KYT system achieving **88.49% performance** on cryptocurrency transaction risk classification. The **SVM (Support Vector Machine)** emerged as the champion model, demonstrating superior performance in distinguishing illicit from licit Bitcoin transactions.\n",
    "\n",
    "#### Key Technical Achievements\n",
    "\n",
    "- **Dimensionality Reduction**: PCA preprocessing reduced feature space from 166 to 59 dimensions while preserving 95% variance\n",
    "- **Algorithm Comparison**: Comprehensive evaluation of 10 ML algorithms with hyperparameter optimization via RandomizedSearchCV  \n",
    "- **Model Ranking**: SVM (88.49%) > KNN (86.87%) > GB (85.02%) demonstrated that ensemble and kernel methods excel in financial pattern recognition\n",
    "- **Pipeline Standardization**: StandardScaler + PCA + model architecture ensures consistent preprocessing across algorithms\n",
    "- **Model Persistence**: All trained models saved with compression for deployment scalability\n",
    "- **Performance Validation**: Stratified cross-validation ensures reliable performance on imbalanced financial data\n",
    "\n",
    "#### Real-World Impact\n",
    "\n",
    "The trained model successfully processed **157,205 unlabeled transactions**, identifying **12,675 potentially illicit transactions**, providing risk assessment capabilities for unknown data‚Äîessential for AML compliance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
