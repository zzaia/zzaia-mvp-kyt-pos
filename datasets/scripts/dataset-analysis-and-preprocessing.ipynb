{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Dataset: Elliptic Data Set\n",
    "\n",
    "**Selection Date**: 2025-08-29  \n",
    "**Selected from**: dataset-exploration-risk-scoring-research.md  \n",
    "**Rank**: #1 out of 10 evaluated datasets  \n",
    "**Suitability Score**: 92/100  \n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "### **Elliptic Data Set**\n",
    "- **Source Platform**: Kaggle\n",
    "- **Direct URL**: https://www.kaggle.com/datasets/ellipticco/elliptic-data-set\n",
    "- **Dataset Size**: 200,000 transactions ÔøΩ 166 features, ~6GB\n",
    "- **Problem Relevance**: High - Bitcoin illicit transaction classification\n",
    "- **Data Quality**: Excellent - professionally curated by Elliptic Co.\n",
    "- **License Type**: Open (with attribution requirements)\n",
    "- **Last Updated**: 2019 (stable reference dataset)\n",
    "- **Preprocessing Needs**: Minimal - ready for ML training\n",
    "\n",
    "## Key Features and Structure\n",
    "\n",
    "### Feature Categories\n",
    "- **Local Features**: 94 transaction-specific features\n",
    "- **Aggregate Features**: 72 neighborhood/graph-based features  \n",
    "- **Total Features**: 166 feature dimensions\n",
    "- **Temporal Component**: Time step information included\n",
    "- **Labels**: Binary classification (illicit/licit)\n",
    "\n",
    "### Specific Features Include:\n",
    "- Transaction fees\n",
    "- Input/output volumes  \n",
    "- Neighbor aggregates\n",
    "- Time steps\n",
    "- BTC amounts\n",
    "- Graph topology metrics\n",
    "- Wallet clustering information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e9b9c",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "Install and import required packages for dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hlbu6h0o8wc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed8199",
   "metadata": {},
   "source": [
    "### Download the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cfdafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset: ellipticco/elliptic-data-set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Download the dataset to the original folder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Download and extract the dataset\u001b[39;00m\n\u001b[32m     12\u001b[39m     kaggle.api.dataset_download_files(\n\u001b[32m     13\u001b[39m         dataset_name, \n\u001b[32m     14\u001b[39m         path=\u001b[38;5;28mstr\u001b[39m(original_dir), \n\u001b[32m     15\u001b[39m         unzip=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset successfully downloaded to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_dir.absolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1465\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1570\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1945\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2182\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2184\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2185\u001b[39m         keep_suspended = \u001b[38;5;28mself\u001b[39m._do_wait_suspend(thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\n\u001b[32m   2187\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2190\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2251\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2252\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m         notify_event.wait(wait_timeout)\n\u001b[32m   2255\u001b[39m         notify_event.clear()\n\u001b[32m   2257\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28mself\u001b[39m._cond.wait(timeout)\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create directory structure for original datasets\n",
    "original_dir = Path(\"../original\")\n",
    "original_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Dataset information\n",
    "dataset_name = \"ellipticco/elliptic-data-set\"\n",
    "print(f\"Downloading dataset: {dataset_name}\")\n",
    "\n",
    "# Download the dataset to the original folder\n",
    "try:\n",
    "    # Download and extract the dataset\n",
    "    kaggle.api.dataset_download_files(\n",
    "        dataset_name, \n",
    "        path=str(original_dir), \n",
    "        unzip=True\n",
    "    )\n",
    "    print(f\"‚úÖ Dataset successfully downloaded to: {original_dir.absolute()}\")\n",
    "    \n",
    "    # List downloaded files\n",
    "    downloaded_files = list(original_dir.glob(\"*\"))\n",
    "    print(f\"\\nDownloaded files ({len(downloaded_files)}):\")\n",
    "    for file_path in downloaded_files:\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024) if file_path.is_file() else 0\n",
    "        print(f\"  - {file_path.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Ensure Kaggle API is configured: ~/.kaggle/kaggle.json\")\n",
    "    print(\"2. Accept the dataset terms on Kaggle website\")\n",
    "    print(\"3. Verify dataset name: ellipticco/elliptic-data-set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a0b2f",
   "metadata": {},
   "source": [
    "## Load and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Elliptic dataset files...\n",
      "\n",
      "         0    1         2         3         4          5         6    \\\n",
      "0  230425980    1 -0.171469 -0.184668 -1.201369  -0.121970 -0.043875   \n",
      "1    5530458    1 -0.171484 -0.184668 -1.201369  -0.121970 -0.043875   \n",
      "2  232022460    1 -0.172107 -0.184668 -1.201369  -0.121970 -0.043875   \n",
      "3  232438397    1  0.163054  1.963790 -0.646376  12.409294 -0.063725   \n",
      "4  230460314    1  1.011523 -0.081127 -1.201369   1.153668  0.333276   \n",
      "5  230459870    1  0.961040 -0.081127 -1.201369   1.303743  0.333276   \n",
      "6  230333930    1 -0.171264 -0.184668 -1.201369  -0.121970 -0.043875   \n",
      "7  230595899    1 -0.171755 -0.184668 -1.201369  -0.046932 -0.043875   \n",
      "8  232013274    1 -0.123127 -0.184668 -1.201369  -0.121970 -0.043875   \n",
      "9  232029206    1 -0.005027  0.578941 -0.091383   4.380281 -0.063725   \n",
      "\n",
      "        7          8         9    ...       157       158       159       160  \\\n",
      "0 -0.113002  -0.061584 -0.162097  ... -0.562153 -0.600999  1.461330  1.461369   \n",
      "1 -0.113002  -0.061584 -0.162112  ...  0.947382  0.673103 -0.979074 -0.978556   \n",
      "2 -0.113002  -0.061584 -0.162749  ...  0.670883  0.439728 -0.979074 -0.978556   \n",
      "3  9.782742  12.414558 -0.163645  ... -0.577099 -0.613614  0.241128  0.241406   \n",
      "4  1.312656  -0.061584 -0.163523  ... -0.511871 -0.400422  0.517257  0.579382   \n",
      "5  1.480381  -0.061584 -0.163577  ... -0.504702 -0.422589 -0.226790 -0.117629   \n",
      "6 -0.113002  -0.061584 -0.161887  ... -0.569626 -0.607306 -0.979074 -0.978556   \n",
      "7 -0.029140  -0.061584 -0.163552  ...  0.969801  0.704641 -0.979074 -0.978556   \n",
      "8 -0.113002  -0.061584 -0.112635  ... -0.128722 -0.235168 -0.979074 -0.978556   \n",
      "9  4.667146   0.851305 -0.163645  ... -0.577099 -0.613614  0.241128  0.241406   \n",
      "\n",
      "        161       162       163       164       165       166  \n",
      "0  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
      "1  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
      "2 -0.098889 -0.106715 -0.131155 -0.183671 -0.120613 -0.119792  \n",
      "3  1.072793  0.085530 -0.131155  0.677799 -0.120613 -0.119792  \n",
      "4  0.018279  0.277775  0.326394  1.293750  0.178136  0.179117  \n",
      "5  0.018279  0.277775  0.413931  1.149556 -0.696053 -0.695540  \n",
      "6  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
      "7  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
      "8 -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399  \n",
      "9  0.604120  0.008632 -0.131155  0.333211 -0.120613 -0.119792  \n",
      "\n",
      "[10 rows x 167 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df_features.head(\u001b[32m10\u001b[39m))\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# print(f\"‚úÖ Features dataset loaded: {df_features.shape}\")\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# print(\"First 10 rows of transaction features:\")\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# print(\"-\" * 80)\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Load transaction classes/labels (elliptic_txs_classes.csv)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m classes_file = original_dir / \u001b[33m\"\u001b[39m\u001b[33melliptic_txs_classes.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m classes_file.exists():\n\u001b[32m     21\u001b[39m     df_classes = pd.read_csv(classes_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1465\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1570\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1945\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2182\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2184\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2185\u001b[39m         keep_suspended = \u001b[38;5;28mself\u001b[39m._do_wait_suspend(thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\n\u001b[32m   2187\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2190\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2251\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2252\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m         notify_event.wait(wait_timeout)\n\u001b[32m   2255\u001b[39m         notify_event.clear()\n\u001b[32m   2257\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28mself\u001b[39m._cond.wait(timeout)\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/venv-analytics/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load and explore the Elliptic dataset\n",
    "# Load the dataset files\n",
    "original_dir = Path(\"../original/elliptic_bitcoin_dataset\")\n",
    "\n",
    "print(\"Loading Elliptic dataset files...\\n\")\n",
    "\n",
    "# Load the main dataset files (based on typical Elliptic dataset structure)\n",
    "try:\n",
    "    # Load transaction features (elliptic_txs_features.csv)\n",
    "    features_file = original_dir / \"elliptic_txs_features.csv\"\n",
    "    if features_file.exists():\n",
    "        df_features = pd.read_csv(features_file, header=None)\n",
    "        print(f\"‚úÖ Features dataset loaded: {df_features.shape}\")\n",
    "        print(\"First 10 rows of transaction features:\")\n",
    "        print(df_features.head(10))\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Load transaction classes/labels (elliptic_txs_classes.csv)\n",
    "    classes_file = original_dir / \"elliptic_txs_classes.csv\"\n",
    "    if classes_file.exists():\n",
    "        df_classes = pd.read_csv(classes_file)\n",
    "        print(f\"\\n‚úÖ Classes dataset loaded: {df_classes.shape}\")\n",
    "        print(\"First 10 rows of transaction classes:\")\n",
    "        print(df_classes.head(10))\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Load transaction edges/relationships (elliptic_txs_edgelist.csv)\n",
    "    edges_file = original_dir / \"elliptic_txs_edgelist.csv\"\n",
    "    if edges_file.exists():\n",
    "        df_edges = pd.read_csv(edges_file)\n",
    "        print(f\"\\n‚úÖ Edges dataset loaded: {df_edges.shape}\")\n",
    "        print(\"First 10 rows of transaction edges:\")\n",
    "        print(df_edges.head(10))\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Summary of all datasets\n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    if 'df_features' in locals():\n",
    "        print(f\"  - Features: {df_features.shape[0]:,} transactions √ó {df_features.shape[1]} features\")\n",
    "    if 'df_classes' in locals():\n",
    "        print(f\"  - Classes: {df_classes.shape[0]:,} labeled transactions\")\n",
    "    if 'df_edges' in locals():\n",
    "        print(f\"  - Edges: {df_edges.shape[0]:,} transaction relationships\")\n",
    "    \n",
    "    # Check class distribution if classes are available\n",
    "    if 'df_classes' in locals():\n",
    "        print(f\"\\nüè∑Ô∏è Class Distribution:\")\n",
    "        class_counts = df_classes.iloc[:, 1].value_counts() if df_classes.shape[1] > 1 else df_classes.iloc[:, 0].value_counts()\n",
    "        for class_label, count in class_counts.items():\n",
    "            print(f\"  - {class_label}: {count:,} transactions ({count/len(df_classes)*100:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading datasets: {e}\")\n",
    "    print(\"\\nAvailable files in original directory:\")\n",
    "    for file_path in original_dir.glob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            print(f\"  - {file_path.name}\")\n",
    "    print(\"\\nTrying to load any CSV files found...\")\n",
    "    \n",
    "    # Fallback: load any CSV files found\n",
    "    csv_files = list(original_dir.glob(\"*.csv\"))\n",
    "    for i, csv_file in enumerate(csv_files):\n",
    "        try:\n",
    "            df_temp = pd.read_csv(csv_file)\n",
    "            print(f\"\\nüìÅ File {i+1}: {csv_file.name}\")\n",
    "            print(f\"Shape: {df_temp.shape}\")\n",
    "            print(\"First 10 rows:\")\n",
    "            print(df_temp.head(10))\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as file_error:\n",
    "            print(f\"‚ùå Could not load {csv_file.name}: {file_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
