{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Dataset: Elliptic Data Set\n",
    "\n",
    "**Selection Date**: 2025-08-29  \n",
    "**Selected from**: dataset-exploration-risk-scoring-research.md  \n",
    "**Rank**: #1 out of 10 evaluated datasets  \n",
    "**Suitability Score**: 92/100  \n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "### **Elliptic Data Set**\n",
    "- **Source Platform**: Kaggle\n",
    "- **Direct URL**: https://www.kaggle.com/datasets/ellipticco/elliptic-data-set\n",
    "- **Dataset Size**: 200,000 transactions ÔøΩ 166 features, ~697.46MB\n",
    "- **Problem Relevance**: High - Bitcoin illicit transaction classification\n",
    "- **Data Quality**: Excellent - professionally curated by Elliptic Co.\n",
    "- **License Type**: Open (with attribution requirements)\n",
    "- **Last Updated**: 2019 (stable reference dataset)\n",
    "- **Preprocessing Needs**: Minimal - ready for ML training\n",
    "\n",
    "### Key Features and Structure\n",
    "\n",
    "### Feature Categories\n",
    "- **Local Features**: 94 transaction-specific features\n",
    "- **Aggregate Features**: 72 neighborhood/graph-based features  \n",
    "- **Total Features**: 166 feature dimensions\n",
    "- **Temporal Component**: Time step information included\n",
    "- **Labels**: Binary classification (illicit/licit)\n",
    "\n",
    "### Specific Features Include:\n",
    "- Transaction fees\n",
    "- Input/output volumes  \n",
    "- Neighbor aggregates\n",
    "- Time steps\n",
    "- BTC amounts\n",
    "- Graph topology metrics\n",
    "- Wallet clustering information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e9b9c",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hlbu6h0o8wc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from azure_utils import AzureBlobDownloader \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set renderer for VS Code compatibility\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "azureClient = AzureBlobDownloader(\"https://stmvppos.blob.core.windows.net\", \"mvpkytsup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed8199",
   "metadata": {},
   "source": [
    "### Dataset Visualization Strategy\n",
    "\n",
    "Let's visualize the dataset in order to check which pre-process to apply:\n",
    " 1. Create the dataset directory and download the original data from source\n",
    " 2. Load all datasets\n",
    " 3. Display the feature dataset\n",
    " 4. Display the classes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc62cb",
   "metadata": {},
   "source": [
    "**1. Create the dataset directory and download the original data from source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfdafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download dataset from Azure Blob Storage...\n",
      "Successfully downloaded 3 files from Azure Blob Storage\n",
      "Download from Azure Blob Storage completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Azure Blob Storage or Kaggle if not already present\n",
    "dataset_str  = \"elliptic_bitcoin_dataset\"\n",
    "dataset_dir = Path(dataset_str)\n",
    "original_str  = \"original\"\n",
    "original_dir = Path(\"../original\")\n",
    "specific_dir = original_dir / dataset_dir\n",
    "\n",
    "if(specific_dir.exists() and any(specific_dir.iterdir())):\n",
    "    print(f\"Dataset already exists in {original_dir}, skipping download.\")\n",
    "else:\n",
    "    print(\"Attempting to download dataset from Azure Blob Storage...\")\n",
    "\n",
    "    if azureClient.download_documents(dataset_str):\n",
    "        print(\"Download from Azure Blob Storage completed successfully.\")\n",
    "    else:\n",
    "        print(\"Falling back to Kaggle download...\")\n",
    "        original_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Download from Kaggle as a fallback\n",
    "        dataset_name = \"ellipticco/elliptic-data-set\"\n",
    "        kaggle.api.dataset_download_files(\n",
    "            dataset_name, \n",
    "            path=str(original_dir), \n",
    "            unzip=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0ea36",
   "metadata": {},
   "source": [
    "**2. Load all datasets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = specific_dir / \"elliptic_txs_features.csv\"\n",
    "if features_file.exists(): df_features = pd.read_csv(features_file, header=None)\n",
    "else: df_features = pd.DataFrame()\n",
    "classes_file = specific_dir / \"elliptic_txs_classes.csv\"\n",
    "if classes_file.exists(): df_classes = pd.read_csv(classes_file)\n",
    "else: df_classes = pd.DataFrame()\n",
    "edges_file = specific_dir / \"elliptic_txs_edgelist.csv\"\n",
    "if edges_file.exists(): df_edges = pd.read_csv(edges_file)\n",
    "else: df_edges = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cb08a",
   "metadata": {},
   "source": [
    "**3. Display the feature dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all datasets\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  - Features: {df_features.shape[0]:,} transactions √ó {df_features.shape[1]} features\")\n",
    "print(f\"  - Classes: {df_classes.shape[0]:,} labeled transactions\")\n",
    "print(f\"  - Edges: {df_edges.shape[0]:,} transaction relationships\")\n",
    "\n",
    "# Try Plotly table with explicit display\n",
    "try:\n",
    "    print(\"\\nüìã Interactive Plotly Table (first 100 rows, first 10 columns):\")\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=list(df_features.columns[:10]),\n",
    "            fill_color='paleturquoise',\n",
    "            align='left'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df_features.iloc[:10000, i] for i in range(10)],\n",
    "            fill_color='lavender',\n",
    "            align='left'\n",
    "        )\n",
    "    )])\n",
    "    fig.update_layout(title=\"Features Dataset - Interactive Table\")\n",
    "    \n",
    "    # Multiple display attempts for VS Code compatibility\n",
    "    fig.show(renderer=\"plotly_mimetype+notebook\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Plotly display failed: {e}\")\n",
    "    print(\"Using pandas display instead:\")\n",
    "    display(df_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cad0f1",
   "metadata": {},
   "source": [
    "**4. Display the classes and edges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data using pandas\n",
    "print(\"\\nüîç Classes Dataset Sample:\")\n",
    "display(df_classes.head(10))\n",
    "\n",
    "print(\"\\nüîç Edges Dataset Sample:\")\n",
    "display(df_edges.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135b9ea",
   "metadata": {},
   "source": [
    "### Dataset Analysis Strategy\n",
    "\n",
    "Let's analyze the following: \n",
    "1. Check if the data is already in standard scale format\n",
    "2. Check for dataset class balance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7eabf",
   "metadata": {},
   "source": [
    "**1. Check if the data is already in standard scale format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if df_features is in standard scale format (mean ‚âà 0, std ‚âà 1)\n",
    "numerical_features = df_features.iloc[:, 1:]  # Exclude txId column\n",
    "means = numerical_features.mean()\n",
    "stds = numerical_features.std()\n",
    "\n",
    "# Standardization check\n",
    "is_standardized = (abs(means) < 0.1).all() and ((stds > 0.9) & (stds < 1.1)).all()\n",
    "\n",
    "print(f\"üìä Feature Standardization Check:\")\n",
    "print(f\"  Mean range: [{means.min():.3f}, {means.max():.3f}]\")\n",
    "print(f\"  Std range: [{stds.min():.3f}, {stds.max():.3f}]\")\n",
    "print(f\"  Is standardized: {'‚úÖ Yes' if is_standardized else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c65b63",
   "metadata": {},
   "source": [
    "**2. Check for dataset class balance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vumvgmozrd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance analysis\n",
    "class_counts = df_classes['class'].value_counts()\n",
    "labeled_only = class_counts[class_counts.index != 'unknown']\n",
    "imbalance_ratio = labeled_only.max() / labeled_only.min() if len(labeled_only) >= 2 else 1.0\n",
    "\n",
    "print(f\"‚öñÔ∏è Class Balance: {len(df_classes):,} total samples\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"  {cls}: {count:,} ({count/len(df_classes)*100:.1f}%)\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"  Status: {'‚úÖ Balanced' if imbalance_ratio <= 1.5 else '‚ö†Ô∏è Imbalanced' if imbalance_ratio <= 3.0 else '‚ùå Highly imbalanced'}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['lightblue', 'orange', 'lightcoral'])\n",
    "ax1.set_title('Class Counts')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "class_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['lightblue', 'orange', 'lightcoral'])\n",
    "ax2.set_title('Class Distribution')\n",
    "ax2.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1339b5b",
   "metadata": {},
   "source": [
    "### Dataset Preparations Strategy\n",
    "\n",
    "Let's prepare the data to training steps:\n",
    "1. Filter transactions to include only labeled data (classes 1 and 2)\n",
    "2. Compress and save the processed dataset for next steps\n",
    "3. Update external data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7af085",
   "metadata": {},
   "source": [
    "**1. Filter transactions to include only labeled data (classes 1 and 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce503ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete dataset\n",
    "print(\"Complete dataset\")\n",
    "df_complete = df_features.merge(df_classes, left_on=df_features.columns[0], right_on=df_classes.columns[0], how='inner')\n",
    "df_complete = df_complete.drop('txId', axis=1)\n",
    "df_complete = df_complete.rename(columns={df_complete.columns[0]: 'txId'})\n",
    "display(df_complete.head(10))\n",
    "\n",
    "# Display labeled dataset\n",
    "print(\"Labeled dataset\")\n",
    "labeledRowSelector = df_complete['class'].isin(['1', '2'])\n",
    "df_labeled = df_complete[labeledRowSelector].copy().reset_index(drop=True)\n",
    "display(df_labeled.head(10))\n",
    "\n",
    "# Display unlabeled dataset\n",
    "print(\"Unlabeled dataset\")\n",
    "unlabeledRowSelector = df_complete['class'] == 'unknown'\n",
    "df_unlabeled = df_complete[unlabeledRowSelector].copy().reset_index(drop=True)\n",
    "display(df_unlabeled.head(10))\n",
    "\n",
    "# Display edges dataset with renamed columns\n",
    "print(\"Edges dataset\")\n",
    "df_edges = df_edges.rename(columns={df_edges.columns[0]: 'source', df_edges.columns[1]: 'destination'})\n",
    "display(df_edges.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10dc6d",
   "metadata": {},
   "source": [
    "**2. Compress and save the processed dataset for next steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5080a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = Path(\"../processed\") / dataset_dir\n",
    "if(processed_dir.exists() == False): processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save df_complete to HDF5 with Blosc compression\n",
    "df_complete.to_hdf(processed_dir / \"df_complete.h5\", key=\"df_complete\", \n",
    "                   complevel=5, complib=\"blosc\", format=\"table\")\n",
    "print(f\"df_complete saved: {df_complete.shape}\")\n",
    "\n",
    "# Save df_labeled to HDF5 with Blosc compression\n",
    "df_labeled.to_hdf(processed_dir / \"df_labeled.h5\", key=\"df_labeled\", \n",
    "                   complevel=5, complib=\"blosc\", format=\"table\")\n",
    "print(f\"df_labeled saved: {df_labeled.shape}\")\n",
    "\n",
    "# Save df_unlabeled to HDF5 with Blosc compression\n",
    "df_unlabeled.to_hdf(processed_dir / \"df_unlabeled.h5\", key=\"df_unlabeled\", \n",
    "                   complevel=5, complib=\"blosc\", format=\"table\")\n",
    "print(f\"df_unlabeled saved: {df_unlabeled.shape}\")\n",
    "\n",
    "# Save df_edges to HDF5 with Blosc compression\n",
    "df_edges.to_hdf(processed_dir / \"df_edges.h5\", key=\"df_edges\", \n",
    "                   complevel=5, complib=\"blosc\", format=\"table\")\n",
    "print(f\"df_edges saved: {df_edges.shape}\")\n",
    "\n",
    "#TODO: Save all data frames in azure blob storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f127f",
   "metadata": {},
   "source": [
    "**3. Update external data warehouse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20543cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
